#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) IBM Corporation 2025
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import, division, print_function
__metaclass__ = type

DOCUMENTATION = r"""
module: zos_replace
version_added: '1.15.0'
short_description: Replace all instances of a pattern within a file or data set.
description:
  - The module L(zos_replace.,/zos_replace.html) can replace all instances of a pattern data set.
author:
  - "Marcel Gutierrez (@AndreMarcel99)"
options:
  after:
    description:
      - If specified, only content after this match will be replaced/removed.
      - Can be used in combination with I(before).
    required: false
    type: str
  backup:
    description:
      - Specifies whether a backup of destination should be created before
        editing the source I(target).
      - When set to C(true), the module creates a backup file or data set.
      - The backup file name will be returned on either success or failure of
        module execution such that data can be retrieved.
      - Use generation data set (GDS) relative positive name. e.g. I(SOME.CREATION(+1)).
    required: false
    type: bool
    default: false
  backup_name:
    description:
      - Specify the USS file name or data set name for the destination backup.
      - If the source I(src) is a USS file or path, the backup_name name must be a file
        or path name, and the USS file or path must be an absolute path name.
      - If the source is an MVS data set, the backup_name name must be an MVS
        data set name, and the dataset must not be preallocated.
      - If the backup_name is not provided, the default backup_name name will
        be used. If the source is a USS file or path, the name of the backup
        file will be the source file or path name appended with a
        timestamp, e.g. C(/path/file_name.2020-04-23-08-32-29-bak.tar).
      - If the source is an MVS data set, it will be a data set with a random
        name generated by calling the ZOAU API. The MVS backup data set
        recovery can be done by renaming it.
      - If I(src) is a data set member and backup_name is not provided, the data set
        member will be backed up to the same partitioned data set with a randomly generated
        member name.
    required: false
    type: str
  before:
    description:
      - If specified, only content before this match will be replaced/removed.
      - Can be used in combination with I(after).
    required: false
    type: str
  encoding:
    description:
      - The character set of the source I(target). L(zos_replace,./zos_replace.html)
        requires it to be provided with correct encoding to read the content
        of a USS file or data set. If this parameter is not provided, this
        module assumes that USS file or data set is encoded in IBM-1047.
      - Supported character sets rely on the charset conversion utility (iconv)
        version; the most common character sets are supported.
    required: false
    type: str
    default: IBM-1047
  target:
    description:
      - The location can be a UNIX System Services (USS) file,
        PS (sequential data set), member of a PDS or PDSE, PDS, PDSE.
      - The USS file must be an absolute pathname.
      - Generation data set (GDS) relative name of generation already
        created. e.g. I(SOME.CREATION(-1)).
    required: true
    type: str
    aliases: [ src, path, destfile ]
  tmp_hlq:
    description:
      - Override the default high level qualifier (HLQ) for temporary and backup
        datasets.
      - The default HLQ is the Ansible user used to execute the module and if
        that is not available, then the value C(TMPHLQ) is used.
    required: false
    type: str
  regexp:
    description:
      - The regular expression to look for in the contents of the file.
    required: true
    type: str
  replace:
    description:
      - The string to replace regexp matches.
      - If not set, matches are removed entirely.
    required: false
    type: str
    default: ""
"""

EXAMPLES = r"""

"""

RETURN = r"""

"""

import os
import re
import io
import codecs
import shutil
import tempfile
import traceback
from ansible.module_utils.basic import AnsibleModule

from ansible_collections.ibm.ibm_zos_core.plugins.module_utils import (
    better_arg_parser,
    data_set,
    backup as Backup
)

from ansible_collections.ibm.ibm_zos_core.plugins.module_utils.import_handler import (
    ZOAUImportError,
)

try:
    from zoautil_py import datasets, zoau_io
except Exception:
    datasets = ZOAUImportError(traceback.format_exc())
    zoau_io = ZOAUImportError(traceback.format_exc())


def resolve_src_name(module, name):
    """Function to solve and validate the exist of the dataset or uss file.

    Parameters
    ----------
        module : object
            Ansible object to execute commands.
        name : str
            Name of the src

    Returns
    ----------
        str: Name of the src.

    """
    if "/" in name:
        if os.path.exists(name):
            if os.path.isfile(name):
                return name
            else:
                module.fail_json(msg=f"Target {name} uss need to be a file not folder.")
        else:
            module.fail_json(msg=f"File {name} uss does not exists.")
    else:
        try:
            dataset = data_set.MVSDataSet(
                name=name
            )
        except Exception:
            messageDict = dict(msg="Unable to resolve name of data set {name}.")
            module.fail_json(**messageDict)

        ds_utils = data_set.DataSetUtils(name)
        if not ds_utils.exists():
            module.fail_json(msg=f"{name} does NOT exist.")

        return dataset.name


def replace_text(content, regexp, replace):
    """Function received the test to work on it to find the regexp
    expected to be replaced.

    Parameters
    ----------
        content : list
            The part or full text to be modified
        regexp : str
            The str that will be search to be replaced
        replace : str
            The str to replace on the text

    Returns
    ----------
        list: the new array of lines with the content replaced
    """
    full_text = "\n".join(content)

    pattern = re.compile(regexp, re.MULTILINE | re.DOTALL)
    modified_text = pattern.sub(replace, full_text)

    return modified_text.split("\n")


def merge_text(original, replace, begin, end):
    """Function to generate the new full text with the replace
        text inside of it.
    Args
    ----------
        original : list
            Tue full original text on list
        replace : list
            The fraction of text that was replace
        begin : int
            Position of the list where start the replace of array
        end : int
            Position of the list where end the replace of array

    Returns
    ----------
        list : The full text on list mode
    """
    if begin == 0 and end != len(original) - 1:
        tail_content = original[end + 1:]
        replace.extend(tail_content)
        return replace

    elif end == len(original) - 1:
        head_content = original[:begin]
        head_content.extend(replace)
        return head_content

    else:
        head_content = original[:begin]
        tail_content = original[end + 1:]
        head_content.extends(replace)
        head_content.extends(tail_content)
        return head_content


def open_uss(file, encoding):
    """Open a uss file on byte mode and decode properly for modifications.

    Args
    ----------
        file : str
            Path to the src of the information
        encoding : str
            Encoding to be use on the decode

    Returns
    ----------
        list : Full text of src decoded on encoding expected.
    """
    lines = []
    with io.open(file, "rb") as content_file:
        for byte_line in content_file:
            line = codecs.decode(byte_line, encoding)
            lines.append(line)
    return lines


def replace_uss(file, regexp, replace, encoding="cp1047", after="", before=""):
    """Function to extract from the uss a fragment or the full text to be replaced and replace the content.

    Args
    ----------
        file : str
            Uss path.
        regexp : str
            Regex expression to search.
        replace : str
            Regex expression or text to replace.
        encoding : str, optional
            Encoding to use en decoding content.
            Defaults to "cp1047".
        after : str, optional
            Str or regex to search where to start the section to replace on the text.
            Defaults to "".
        before :str, optional
            Str or regex to search where to end the section to replace on the text.
            Defaults to "".

    Returns
    ----------
        list : List with the new text with the replace expected.
    """
    decode_list = []
    for line in open_uss(file=file, encoding=encoding):
        decode_list.append(line)

    if not bool(after) and not bool(before):
        return replace_text(content=decode_list, regex=regexp, replace=replace)

    pattern_begin = re.compile(after, re.DOTALL) if after else after
    pattern_end = re.compile(before, re.DOTALL) if before else before

    begin_block_code = 0
    end_block_code = len(decode_list) - 1

    line_counter = 0
    search_after = bool(after)
    search_before = False if search_after else True

    for line in decode_list:
        if search_after and pattern_begin.match(line) is not None:
            begin_block_code = line_counter
            if not before:
                break
            else:
                search_before = True
                search_after = False

        if search_before and pattern_end.match(line) is not None:
                end_block_code = line_counter + 1
                break
        line_counter += 1

    new_text = replace_text(decode_list[begin_block_code: end_block_code], regex=regexp, replace=replace)
    full_new_text = merge_text(original=decode_list, replace=new_text, begin=begin_block_code, end=end_block_code)
    return full_new_text


def replace_ds(ds, regexp, replace, encoding="cp1047", after="", before=""):
    """Function to extract from the uss a fragment or the full text to be replaced and replace the content.

    Args
    ----------
        ds : str
            data set name.
        regexp : str
            Regex expression to search.
        replace : str
            Regex expression or text to replace.
        encoding : str, optional
            Encoding to use en decoding content.
            Defaults to "cp1047".
        after : str, optional
            Str or regex to search where to start the section to replace on the text.
            Defaults to "".
        before :str, optional
            Str or regex to search where to end the section to replace on the text.
            Defaults to "".

    Returns
    ----------
        list : List with the new text with the replace expected.
    """
    with zoau_io.RecordIO(f"//'{ds}'") as dataset_read:
        dataset_content = dataset_read.readrecords()

    decode_list = [codecs.decode(record, encoding) for record in dataset_content]

    if not bool(after) and not bool(before):
        return replace_text(content=decode_list, regex=regexp, replace=replace)

    pattern_begin = re.compile(after, re.DOTALL) if after else after
    pattern_end = re.compile(before, re.DOTALL) if before else before

    begin_block_code = 0
    end_block_code = len(decode_list) - 1

    line_counter = 0
    search_after = True if after else False
    search_before = False if search_after else True

    for line in decode_list:
        if search_after and pattern_begin.match(line) is not None:
            begin_block_code = line_counter + 1
            if not before:
                break
            else:
                search_before = True
                search_after = False

        if search_before and pattern_end.match(line) is not None:
                end_block_code = line_counter + 1
                break
        line_counter += 1

    new_text = replace_text(decode_list[begin_block_code: end_block_code], regex=regexp, replace=replace)
    full_new_text = merge_text(original=decode_list, replace=new_text, begin=begin_block_code, end=end_block_code)
    return full_new_text


def run_module():
    module = AnsibleModule(
        argument_spec=dict(
            after=dict(type='str'),
            backup=dict(type='bool', default=False, required=False),
            backup_name=dict(type='str', default=None, required=False),
            before=dict(type='str'),
            encoding=dict(type='str', default='IBM-1047', required=False),
            target=dict(type="str", required=True, aliases=['src', 'path', 'destfile']),
            tmp_hlq=dict(type='str', required=False, default=None),
            regexp=dict(type="str", required=True),
            replace=dict(type='str', default=""),
        ),
        supports_check_mode=False
    )
    args_def = dict(
        after=dict(type='str'),
        backup=dict(type='bool', default=False, required=False),
        backup_name=dict(type='data_set_or_path', default=None, required=False),
        before=dict(type='str'),
        encoding=dict(type='str', default='IBM-1047', required=False),
        target=dict(type="data_set_or_path", required=True, aliases=['src', 'path', 'destfile']),
        tmp_hlq=dict(type='qualifier_or_empty', required=False, default=None),
        regexp=dict(type="str", required=True),
        replace=dict(type='str', default=""),
    )

    try:
        parser = better_arg_parser.BetterArgParser(args_def)
        parsed_args = parser.parse_args(module.params)
        module.params = parsed_args
    except ValueError as err:
        module.fail_json(
            msg='Parameter verification failed.',
            stderr=str(err)
        )

    src = module.params.get("target")
    src = resolve_src_name(module=module, name=src)
    uss = True if "/" in src else False
    after = module.params.get("after")
    before = module.params.get("before")
    regexp = module.params.get("regexp")
    replace = module.params.get("replace")
    encoding = module.params.get("encoding")
    backup = module.params.get("backup")
    tmphlq = parsed_args.get('tmp_hlq')
    if parsed_args.get('backup_name') and backup:
        backup = parsed_args.get('backup_name')

    result = dict()

    if backup:
        if isinstance(backup, bool):
            backup = None
        try:
            if uss:
                result['backup_name'] = Backup.uss_file_backup(src, backup_name=backup, compress=False)
            else:
                result['backup_name'] = Backup.mvs_file_backup(dsn=src, bk_dsn=backup, tmphlq=tmphlq)
        except Exception as err:
            module.fail_json(msg=f"Unable to allocate backup {backup} destination: {str(err)}")

    if uss:
        full_text = replace_uss(file=src, regexp=regexp, replace=replace, encoding=encoding, after=after, before=before)
        tmp_file = tempfile.NamedTemporaryFile(delete=False)
        tmp_file = tmp_file.name
        try:
            with open(tmp_file, 'w') as f:
                for line in full_text:
                    f.write(f"{line}\n")
        except Exception as e:
            os.remove(tmp_file)
            module.fail_json(
                msg=f"Unable to write on data set {src}. {e}",
            )

        try:
            f = open(src, 'r+')
            f.truncate(0)
            shutil.copyfile(tmp_file, src)
        finally:
            os.remove(tmp_file)

    else:
        full_text = replace_ds(ds=src, regexp=regexp, replace=replace, encoding=encoding, after=after, before=before)
        bk_ds = datasets.tmp_name()
        datasets.create(name=bk_ds, dataset_type="SEQ")
        try:
            for line in full_text:
                rc_write = datasets.write(dataset_name=bk_ds, content=line.rstrip(), append=True)
                if rc_write != 0:
                    raise Exception("Non zero return code from datasets.write.")
        except Exception as e:
            datasets.delete(dataset=bk_ds)
            module.fail_json(
                msg=f"Unable to write on data set {src}. {e}",
            )

        try:
            datasets.copy(source=bk_ds, target=src)
        finally:
            datasets.delete(dataset=bk_ds)

    result["target"] = src

def main():
    run_module()


if __name__ == '__main__':
    main()
