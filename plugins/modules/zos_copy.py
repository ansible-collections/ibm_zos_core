#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) IBM Corporation 2019, 2020, 2021
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import, division, print_function
from ansible_collections.ibm.ibm_zos_core.plugins.module_utils.import_handler import (
    MissingZOAUImport,
)
from ansible_collections.ibm.ibm_zos_core.plugins.module_utils.mvs_cmd import (
    idcams, iebcopy, ikjeft01
)
from ansible_collections.ibm.ibm_zos_core.plugins.module_utils import (
    better_arg_parser, data_set, encode, vtoc, backup, copy
)
from ansible_collections.ibm.ibm_zos_core.plugins.module_utils.ansible_module import (
    AnsibleModuleHelper,
)
from ansible.module_utils._text import to_bytes
from ansible.module_utils.basic import AnsibleModule
from ansible.module_utils.six import PY3
from re import IGNORECASE
from hashlib import sha256
import glob
import shutil
import stat
import math
import tempfile
import os, sys
import subprocess

__metaclass__ = type


DOCUMENTATION = r"""
---
module: zos_copy
version_added: '1.2.0'
short_description: Copy data to z/OS
description:
  - The M(zos_copy) module copies a file or data set from a local or a
    remote machine to a location on the remote machine.
  - Use the M(zos_fetch) module to copy files or data sets from remote
    locations to the local machine.
author:
  - "Asif Mahmud (@asifmahmud)"
  - "Demetrios Dimatos (@ddimatos)"
options:
  backup:
    description:
      - Specifies whether a backup of destination should be created before
        copying data.
      - When set to C(true), the module creates a backup file or data set.
      - The backup file name will be returned on either success or failure of
        module execution such that data can be retrieved.
    type: bool
    default: false
    required: false
  backup_name:
    description:
      - Specify a unique USS file name or data set name for the destination backup.
      - If the destination (dest) is a USS file or path, the backup_name must
        be a file or path name, and the USS path or file must be an absolute
        path name.
      - If the destination is an MVS data set, the backup_name must be an MVS
        data set name.
      - If the backup_name is not provided, the default backup_name will
        be used. If the destination is a USS file or USS path, the name of the backup
        file will be the destination file or path name appended with a
        timestamp, e.g. C(/path/file_name.2020-04-23-08-32-29-bak.tar).
      - If the destination is an MVS data set, it will be a data set with a random
        name generated by calling the ZOAU API. The MVS backup data set recovery
        can be done by renaming it.
      - If C(dest) is a data set member and C(backup_name) is not provided, the data set
        member will be backed up to the same partitioned data set with a randomly
        generated member name.
    required: false
    type: str
  content:
    description:
      - When used instead of C(src), sets the contents of a file or data set
        directly to the specified value.
      - Works only when C(dest) is a USS file, sequential data set, or a
        partitioned data set member.
      - This is for simple values; for anything complex or with formatting, use
        L(ansible.builtin.copy,https://docs.ansible.com/ansible/latest/modules/copy_module.html)
      - If C(dest) is a directory, then content will be copied to
        C(/path/to/dest/inline_copy).
    type: str
    required: false
  dest:
    description:
      - Remote absolute path or data set where the file should be copied to.
      - Destination can be a USS path or an MVS data set name.
      - If C(dest) is a nonexistent USS file, it will be created.
      - If C(dest) is a nonexistent data set, storage management rules will be
        used to determine the volume where C(dest) will be allocated.
      - If C(src) and C(dest) are files and if the parent directory of C(dest)
        does not exist, then the task will fail.
      - When the C(dest) is an existing VSAM (KSDS) or VSAM (ESDS), then source
        can be ESDS, KSDS or RRDS. The C(dest) will be deleted and storage
        management rules will be used to determine the volume where C(dest) will
        be allocated.
      - When the C(dest) is an existing VSAM (RRDS), then the source must be RRDS.
        The C(dest) will be deleted and storage management rules will be used to
        determine the volume where C(dest) will be allocated.
      - When C(dest) is and existing VSAM (LDS), then source must be LDS. The
        C(dest) will be deleted and storage management rules will be used to
        determine the volume where C(dest) will be allocated.
      - When C(dest) is a data set, you can override storage management rules
        by specifying both C(volume) and other optional DS specs (type, space,
        record size, etc).
    type: str
    required: true
  encoding:
    description:
      - Specifies which encodings the destination file or data set should be
        converted from and to.
      - If C(encoding) is not provided, the module determines which local and remote
        charsets to convert the data from and to. Note that this is only done for text
        data and not binary data.
      - If C(encoding) is provided and C(src) is an MVS data set, task will fail.
      - Only valid if C(is_binary) is false.
    type: dict
    required: false
    suboptions:
      from:
        description:
          - The encoding to be converted from
        required: true
        type: str
      to:
        description:
          - The encoding to be converted to
        required: true
        type: str
  force:
    description:
      - If set to C(true) and the remote file or data set C(dest) is empty,
        the C(dest) will be reused.
      - If set to C(true) and the remote file or data set C(dest) is NOT empty,
        the C(dest) will be deleted and recreated with the C(src) data set
        attributes, otherwise it will be recreated with the C(dest) data set
        attributes.
      - To backup data before any deletion, see parameters C(backup) and
        C(backup_name).
      - If set to C(false), the file or data set will only be copied if the
        destination does not exist.
      - If set to C(false) and destination exists, the module exits with a note to
        the user.
    type: bool
    default: false
    required: false
  ignore_sftp_stderr:
    description:
      - During data transfer through SFTP, the module fails if the SFTP command
        directs any content to stderr. The user is able to override this
        behavior by setting this parameter to C(true). By doing so, the module
        would essentially ignore the stderr stream produced by SFTP and continue
        execution.
      - When Ansible verbosity is set to greater than 3, either through the
        command line interface (CLI) using B(-vvvv) or through environment
        variables such as B(verbosity = 4), then this parameter will
        automatically be set to C(true).
    type: bool
    required: false
    default: false
    version_added: "1.4.0"
  is_binary:
    description:
      - If set to C(true), indicates that the file or data set to be copied is a
        binary file/data set.
    type: bool
    default: false
    required: false
  local_follow:
    description:
      - This flag indicates that any existing filesystem links in the source tree
        should be followed.
    type: bool
    default: true
    required: false
  mode:
    description:
      - The permission of the destination file or directory.
      - If C(dest) is USS, this will act as Unix file mode, otherwise ignored.
      - It should be noted that modes are octal numbers.
        The user must either add a leading zero so that Ansible's YAML parser
        knows it is an octal number (like C(0644) or C(01777))or quote it
        (like C('644') or C('1777')) so Ansible receives a string and can do its
        own conversion from string into number. Giving Ansible a number without
        following one of these rules will end up with a decimal number which
        will have unexpected results.
      - The mode may also be specified as a symbolic mode
        (for example, ``u+rwx`` or ``u=rw,g=r,o=r``) or a special
        string `preserve`.
      - C(preserve) means that the file will be given the same permissions as
        the source file.
    type: str
    required: false
  remote_src:
    description:
      - If set to C(false), the module searches for C(src) at the local machine.
      - If set to C(true), the module goes to the remote/target machine for C(src).
    type: bool
    default: false
    required: false
  sftp_port:
    description:
      - Configuring the SFTP port used by the M(zos_copy) module has been
        deprecated and will be removed in ibm.ibm_zos_core collection version
        1.5.0.
      - Configuring the SFTP port with I(sftp_port) will no longer have any
        effect on which port is used by this module.
      - To configure the SFTP port used for module M(zos_copy), refer to topic
        L(using connection plugins,https://docs.ansible.com/ansible/latest/plugins/connection.html#using-connection-plugins)
      - If C(ansible_port) is not specified, port 22 will be used.
    type: int
    required: false
  src:
    description:
      - Path to a file/directory or name of a data set to copy to remote
        z/OS system.
      - If C(remote_src) is true, then C(src) must be the path to a Unix
        System Services (USS) file, name of a data set, or data set member.
      - If C(src) is a local path or a USS path, it can be absolute or relative.
      - If C(src) is a directory, destination must be a partitioned data set or
        a USS directory.
      - If C(src) is a file and dest ends with "/" or destination is a
        directory, the file is copied to the directory with the same filename as
        src.
      - If C(src) is a VSAM data set, destination must also be a VSAM.
      - Wildcards can be used to copy multiple PDS/PDSE members to another
        PDS/PDSE.
      - Required unless using C(content).
    type: str
  validate:
    description:
      - Specifies whether to perform checksum validation for source and
        destination files.
      - Valid only for USS destination, otherwise ignored.
    type: bool
    required: false
    default: false
  volume:
    description:
      - If C(dest) does not exist, specify which volume C(dest) should be
        allocated to.
      - Only valid when the destination is an MVS data set.
      - The volume must already be present on the device.
      - If no volume is specified, storage management rules will be used to
        determine the volume where C(dest) will be allocated.
      - If the storage administrator has specified a system default unit name
        and you do not set a C(volume) name for non-system-managed data sets,
        then the system uses the volumes associated with the default unit name.
        Check with your storage administrator to determine whether a default
        unit name has been specified.
    type: str
    required: false
  destination_dataset:
    description:
      - These are settings to use when creating the destination data set
    required: false
    type: dict
    suboptions:
      type:
        description:
          - Organization of the destination
        type: str
        required: true
        choices:
          - KSDS
          - ESDS
          - RRDS
          - LDS
          - SEQ
          - PDS
          - PDSE
          - MEMBER
          - BASIC
      space_primary:
        description:
          - If the destination I(dest) data set does not exist , this sets the
            primary space allocated for the data set.
          - The unit of space used is set using I(space_type).
        type: str
        required: false
      space_secondary:
        description:
          - If the destination I(dest) data set does not exist , this sets the
            secondary space allocated for the data set.
          - The unit of space used is set using I(space_type).
        type: str
        required: false
      space_type:
        description:
          - If the destination data set does not exist, this sets the unit of
            measurement to use when defining primary and secondary space.
          - Valid units of size are C(K), C(M), C(G), C(CYL), and C(TRK).
        type: str
        choices:
          - K
          - M
          - G
          - CYL
          - TRK
        required: false
      record_format:
        description:
          - If the destination data set does not exist, this sets the format of the
            data set. (e.g C(FB))
          - Choices are case-insensitive.
        required: false
        choices:
          - FB
          - VB
          - FBA
          - VBA
          - U
        type: str
      record_length:
        description:
          - The length of each record in the data set, in bytes.
          - For variable data sets, the length must include the 4-byte prefix area.
          - "Defaults vary depending on format: If FB/FBA 80, if VB/VBA 137, if U 0."
        type: int
        required: false
      block_size:
        description:
          - The block size to use for the data set.
        type: int
        required: false

notes:
    - Destination data sets are assumed to be in catalog. When trying to copy
      to an uncataloged data set, the module assumes that the data set does
      not exist and will create it.
    - Destination will be backed up if either C(backup) is C(true) or
      C(backup_name) is provided. If C(backup) is C(false) but C(backup_name)
      is provided, task will fail.
    - When copying local files or directories, temporary storage will be used
      on the remote z/OS system. The size of the temporary storage will
      correspond to the size of the file or directory being copied. Temporary
      files will always be deleted, regardless of success or failure of the
      copy task.
    - VSAM data sets can only be copied to other VSAM data sets.
    - For supported character sets used to encode data, refer to the
      L(documentation,https://ibm.github.io/z_ansible_collections_doc/ibm_zos_core/docs/source/resources/character_set.html).
    - M(zos_copy) uses SFTP (Secure File Transfer Protocol) for the underlying
      transfer protocol; Co:Z SFTP is not supported. In the case of Co:z SFTP,
      you can exempt the Ansible userid on z/OS from using Co:Z thus falling back
      to using standard SFTP.
seealso:
- module: zos_fetch
- module: zos_data_set
"""

EXAMPLES = r"""
- name: Copy a local file to a sequential data set
  zos_copy:
    src: /path/to/sample_seq_data_set
    dest: SAMPLE.SEQ.DATA.SET

- name: Copy a local file to a USS location and validate checksum
  zos_copy:
    src: /path/to/test.log
    dest: /tmp/test.log
    validate: true

- name: Copy a local ASCII encoded file and convert to IBM-1047
  zos_copy:
    src: /path/to/file.txt
    dest: /tmp/file.txt

- name: Copy a local directory to a PDSE
  zos_copy:
    src: /path/to/local/dir/
    dest: HLQ.DEST.PDSE

- name: Copy file with permission details
  zos_copy:
    src: /path/to/foo.conf
    dest: /etc/foo.conf
    mode: 0644
    group: foo
    owner: bar

- name: Module will follow the symbolic link specified in src
  zos_copy:
    src: /path/to/link
    dest: /path/to/uss/location
    local_follow: true

- name: Copy a local file to a PDS member and convert encoding
  zos_copy:
    src: /path/to/local/file
    dest: HLQ.SAMPLE.PDSE(MEMBER)
    encoding:
      from: UTF-8
      to: IBM-037

- name: Copy a VSAM (KSDS) to a VSAM (KSDS)
  zos_copy:
    src: SAMPLE.SRC.VSAM
    dest: SAMPLE.DEST.VSAM
    remote_src: true

- name: Copy inline content to a sequential dataset and replace existing data
  zos_copy:
    content: 'Inline content to be copied'
    dest: SAMPLE.SEQ.DATA.SET

- name: Copy a USS file to sequential data set and convert encoding beforehand
  zos_copy:
    src: /path/to/remote/uss/file
    dest: SAMPLE.SEQ.DATA.SET
    remote_src: true

- name: Copy a USS directory to another USS directory
  zos_copy:
    src: /path/to/uss/dir
    dest: /path/to/dest/dir
    remote_src: true

- name: Copy a local binary file to a PDSE member
  zos_copy:
    src: /path/to/binary/file
    dest: HLQ.SAMPLE.PDSE(MEMBER)
    is_binary: true

- name: Copy a sequential data set to a PDS member
  zos_copy:
    src: SAMPLE.SEQ.DATA.SET
    dest: HLQ.SAMPLE.PDSE(MEMBER)
    remote_src: true

- name: Copy a local file and take a backup of the existing file
  zos_copy:
    src: /path/to/local/file
    dest: /path/to/dest
    backup: true
    backup_name: /tmp/local_file_backup

- name: Copy a PDS on remote system to a new PDS
  zos_copy:
    src: HLQ.SRC.PDS
    dest: HLQ.NEW.PDS
    remote_src: true

- name: Copy a PDS on remote system to a PDS, replacing the original
  zos_copy:
    src: HLQ.SAMPLE.PDSE
    dest: HLQ.EXISTING.PDSE
    remote_src: true

- name: Copy PDS member to a new PDS member. Replace if it already exists.
  zos_copy:
    src: HLQ.SAMPLE.PDSE(SRCMEM)
    dest: HLQ.NEW.PDSE(DESTMEM)
    remote_src: true

- name: Copy a USS file to a PDSE member. If PDSE does not exist, allocate it.
  zos_copy:
    src: /path/to/uss/src
    dest: DEST.PDSE.DATA.SET(MEMBER)
    remote_src: true

- name: Copy a sequential data set to a USS file
  zos_copy:
    src: SRC.SEQ.DATA.SET
    dest: /tmp/
    remote_src: true

- name: Copy a PDSE member to USS file
  zos_copy:
    src: SRC.PDSE(MEMBER)
    dest: /tmp/member
    remote_src: true

- name: Copy a PDS to a USS directory (/tmp/SRC.PDS).
  zos_copy:
    src: SRC.PDS
    dest: /tmp
    remote_src: true

- name: Copy all members inside a PDS to another PDS
  zos_copy:
    src: SOME.SRC.PDS(*)
    dest: SOME.DEST.PDS
    remote_src: true

- name: Copy all members starting with 'ABC' inside a PDS to another PDS
  zos_copy:
    src: SOME.SRC.PDS(ABC*)
    dest: SOME.DEST.PDS
    remote_src: true

- name: Allocate destination in a specific volume
  zos_copy:
    src: SOME.SRC.PDS
    dest: SOME.DEST.PDS
    volume: 'VOL033'
    remote_src: true
"""

RETURN = r"""
src:
    description: Source file or data set being copied.
    returned: changed
    type: str
    sample: /path/to/source.log
dest:
    description: Destination file/path or data set name.
    returned: success
    type: str
    sample: SAMPLE.SEQ.DATA.SET
checksum:
    description: SHA256 checksum of the file after running zos_copy.
    returned: C(validate) is C(true) and if dest is USS
    type: str
    sample: 8d320d5f68b048fc97559d771ede68b37a71e8374d1d678d96dcfa2b2da7a64e
backup_name:
    description: Name of the backup file or data set that was created.
    returned: if backup=true or backup_name=true
    type: str
    sample: /path/to/file.txt.2015-02-03@04:15~
gid:
    description: Group id of the file, after execution.
    returned: success and if dest is USS
    type: int
    sample: 100
group:
    description: Group of the file, after execution.
    returned: success and if dest is USS
    type: str
    sample: httpd
owner:
    description: Owner of the file, after execution.
    returned: success and if dest is USS
    type: str
    sample: httpd
uid:
    description: Owner id of the file, after execution.
    returned: success and if dest is USS
    type: int
    sample: 100
mode:
    description: Permissions of the target, after execution.
    returned: success and if dest is USS
    type: str
    sample: 0644
size:
    description: Size(in bytes) of the target, after execution.
    returned: success and dest is USS
    type: int
    sample: 1220
state:
    description: State of the target, after execution.
    returned: success and if dest is USS
    type: str
    sample: file
note:
    description: A note to the user after module terminates.
    returned: C(force) is C(false) and dest exists
    type: str
    sample: No data was copied
msg:
    description: Failure message returned by the module.
    returned: failure
    type: str
    sample: Error while gathering data set information
stdout:
    description: The stdout from a USS command or MVS command, if applicable.
    returned: failure
    type: str
    sample: Copying local file /tmp/foo/src to remote path /tmp/foo/dest
stderr:
    description: The stderr of a USS command or MVS command, if applicable.
    returned: failure
    type: str
    sample: No such file or directory "/tmp/foo"
stdout_lines:
    description: List of strings containing individual lines from stdout.
    returned: failure
    type: list
    sample: [u"Copying local file /tmp/foo/src to remote path /tmp/foo/dest.."]
stderr_lines:
    description: List of strings containing individual lines from stderr.
    returned: failure
    type: list
    sample: [u"FileNotFoundError: No such file or directory '/tmp/foo'"]
rc:
    description: The return code of a USS or MVS command, if applicable.
    returned: failure
    type: int
    sample: 8
cmd:
    description: The MVS command issued, if applicable.
    returned: failure
    type: str
    sample: REPRO INDATASET(SAMPLE.DATA.SET) OUTDATASET(SAMPLE.DEST.DATA.SET)
"""


if PY3:
    from re import fullmatch
else:
    from re import match as fullmatch

try:
    from zoautil_py import datasets
except Exception:
    datasets = MissingZOAUImport()


MVS_PARTITIONED = frozenset({"PE", "PO", "PDSE", "PDS"})
# Underlying code will map both BASIC and SEQ to PS

MVS_SEQ = frozenset({"PS", "SEQ", "BASIC"})


class CopyHandler(object):
    def __init__(
        self,
        module,
        dest_exists,
        is_binary=False,
        backup_name=None
    ):
        """Utility class to handle copying data between two targets

        Arguments:
            module {AnsibleModule} -- The AnsibleModule object from currently
                                      running module
            dest_exists {boolean} -- Whether destination already exists

        Keyword Arguments:
            is_binary {bool} -- Whether the file or data set to be copied
                                contains binary data
            backup_name {str} -- The USS path or data set name of destination
                                 backup
        """
        self.module = module
        self.dest_exists = dest_exists
        self.is_binary = is_binary
        self.backup_name = backup_name

    def fail_json(self, **kwargs):
        """ Wrapper for AnsibleModule.fail_json """
        d = dict(dest_exists=self.dest_exists, backup_name=self.backup_name)
        self.module.fail_json(**self._merge_hash(kwargs, d))

    def run_command(self, cmd, **kwargs):
        """ Wrapper for AnsibleModule.run_command """
        return self.module.run_command(cmd, **kwargs)

    def exit_json(self, **kwargs):
        """ Wrapper for AnsibleModule.exit_json """
        d = dict(dest_exists=self.dest_exists, backup_name=self.backup_name)
        self.module.exit_json(**self._merge_hash(kwargs, d))

    def copy_to_seq(
        self,
        src,
        temp_path,
        conv_path,
        dest,
        src_ds_type
    ):
        """Copy source to a sequential data set.

        Arguments:
            src {str} -- Path to USS file or data set name
            temp_path {str} -- Path to the location where the control node
                               transferred data to
            conv_path {str} -- Path to the converted source file
            dest {str} -- Name of destination data set
            src_ds_type {str} -- The type of source
        """
        new_src = temp_path or conv_path or src

        # # Pre-clear data set: get destination stats before deleting if incoming info is all default
        # if self.dest_exists:
        #     if space_primary is None or space_primary == 5:
        #         if record_length is None or record_length == 80:
        #             if block_size is None:
        #                 res = datasets.listing(dest)
        #                 type = res[0].dsorg
        #                 record_format = res[0].recfm
        #                 record_length = res[0].lrecl
        #                 block_size = res[0].block_size
        #                 space_primary = round(res[0].total_space / 1024)
        #                 space_type = "K"
        #                 space_secondary = 3
        #     try:
        #         data_set.DataSet.delete(dest)
        #         self.dest_exists = False
        #     except Exception as err:
        #         self.fail_json(msg=repr(err))

        # if not self.dest_exists:
        #     parms = dict(
        #         name=dest,
        #         type=type,
        #         space_primary=space_primary,
        #         space_secondary=space_secondary,
        #         space_type=space_type,
        #         record_format=record_format,
        #         record_length=record_length,
        #         block_size=block_size
        #     )
        #     if alloc_vol:
        #         parms['volumes'] = [alloc_vol]

# Demetri here 12/2/21
        # try:
        #     data_set.DataSet.create(**parms)
        # except Exception as err:
        #     self.fail_json(msg=repr(err))

        # self.fail_json(
        #         msg="src_ds_type {0} new_src {1} dest {2}".format(src_ds_type, new_src,dest)
        # )
        # msg": "src_ds_type USS new_src /SYSTEM/tmp/hi.txt dest SOME.GEN.SEQ",

        if src_ds_type == "USS":
            rc, out, err = self.run_command(
                "cp {0} {1} \"//'{2}'\"".format(
                    "-B" if self.is_binary else "", new_src, dest
                )
            )
            if rc != 0:
                self.fail_json(
                    msg="Unable to copy source {0} to {1}".format(src, dest),
                    rc=rc,
                    stderr=err,
                    stdout=out,
                )
        else:
            # *****************************************************************
            # When Copying a PDSE member to a non-existent sequential data set
            # using cp "//'SOME.PDSE.DATA.SET(MEMBER)'" "//'SOME.DEST.SEQ'",
            # An I/O abend could be trapped and can be resolved by allocating
            # the destination data set before copying.
            # *****************************************************************

            response = datasets._copy(new_src, dest)
            if response.rc != 0:
                self.fail_json(
                    msg="Unable to copy source {0} to {1}".format(
                        new_src, dest),
                    rc=response.rc,
                    stdout=response.stdout_response,
                    stderr=response.stderr_response
                )

    def copy_to_vsam(self, src, dest, alloc_vol):
        """ Copy source VSAM to destination VSAM. If source VSAM exists, then
        it will be deleted and a new VSAM cluster will be allocated.

        Arguments:
            src {str} -- The name of the source VSAM
            dest {str} -- The name of the destination VSAM
            alloc_vol {str} -- The volume where the destination should be allocated
        """
        if self.dest_exists:
            response = datasets._delete(dest)
            if response.rc != 0:
                self.fail_json(
                    msg="Unable to delete destination data set {0}".format(
                        dest),
                    rc=response.rc,
                    stdout=response.stdout_response,
                    stderr=response.stderr_response
                )
        self.allocate_model(dest, src, vol=alloc_vol)

        repro_cmd = """  REPRO -
        INDATASET({0}) -
        OUTDATASET({1})""".format(src, dest)
        rc, out, err = idcams(repro_cmd, authorized=True)
        if rc != 0:
            self.fail_json(
                msg=("IDCAMS REPRO encountered a problem while "
                     "copying {0} to {1}".format(src, dest)),
                stdout=out,
                stderr=err,
                rc=rc,
                stdout_lines=out.splitlines(),
                stderr_lines=err.splitlines(),
                cmd=repro_cmd,
            )

    def convert_encoding(self, src, temp_path, encoding):
        """Convert encoding for given src

        Arguments:
            src {str} -- Path to the USS source file or directory
            temp_path {str} -- Path to the location where the control node
                               transferred data to
            encoding {dict} -- Charsets that the source is to be converted
                               from and to

        Raises:
            EncodingConversionError -- When the encoding of a USS file is not
                                       able to be converted

        Returns:
            {str} -- The USS path where the converted data is stored
        """
        from_code_set = encoding.get("from")
        to_code_set = encoding.get("to")
        enc_utils = encode.EncodeUtils()
        new_src = temp_path or src

        if os.path.isdir(new_src):
            if temp_path:
                if src.endswith("/"):
                    new_src = "{0}/{1}".format(
                        temp_path, os.path.basename(os.path.dirname(src))
                    )
                else:
                    new_src = "{0}/{1}".format(temp_path,
                                               os.path.basename(src))
            try:
                if not temp_path:
                    temp_dir = tempfile.mkdtemp()
                    shutil.copytree(new_src, temp_dir)
                    new_src = temp_dir
                self._convert_encoding_dir(new_src, from_code_set, to_code_set)
                self._tag_file_encoding(new_src, to_code_set, is_dir=True)

            except Exception as err:
                shutil.rmtree(new_src)
                self.fail_json(msg=str(err))
        else:
            try:
                if not temp_path:
                    fd, temp_src = tempfile.mkstemp()
                    os.close(fd)
                    shutil.copy(new_src, temp_src)
                    new_src = temp_src

                rc = enc_utils.uss_convert_encoding(
                    new_src,
                    new_src,
                    from_code_set,
                    to_code_set
                )
                if not rc:
                    raise EncodingConversionError(
                        new_src,
                        from_code_set,
                        to_code_set
                    )
                self._tag_file_encoding(new_src, to_code_set)

            except Exception as err:
                os.remove(new_src)
                self.fail_json(msg=str(err))
        return new_src

    def allocate_model(self, ds_name, model, vol=None):
        """Use 'model' data sets allocation paramters to allocate the given
        data set.

        Arguments:
            ds_name {str} -- The name of the data set to allocate
            model {str} -- The name of the data set whose allocation parameters
            should be used to allocate 'ds_name'
            dsntype {str} -- The type of data set to be allocated
            vol {str} -- The volume where data set should be allocated

        Returns:
            {int} -- The return code of executing the allocation command
        """
        blksize = data_set.DataSetUtils(model).blksize()

        alloc_cmd = """  ALLOC DS('{0}') -
    LIKE('{1}') -
    {2}{3}""".format(
            ds_name,
            model,
            "BLKSIZE({0}) ".format(blksize) if blksize else "",
            "VOLUME({0})".format(vol.upper()) if vol else ""
        )

        rc, out, err = ikjeft01(alloc_cmd, authorized=True)
        if rc != 0:
            self.fail_json(
                msg="Unable to allocate destination {0}".format(ds_name),
                stdout=out,
                stderr=err,
                rc=rc,
                stdout_lines=out.splitlines(),
                stderr_lines=err.splitlines(),
                cmd=alloc_cmd,
            )
        return rc

    def _convert_encoding_dir(self, dir_path, from_code_set, to_code_set):
        """Convert encoding for all files inside a given directory

        Arguments:
            dir_path {str} -- Absolute path to the input directory
            from_code_set {str} -- The character set to convert the files from
            to_code_set {str} -- The character set to convert the files to

        Raises
            EncodingConversionError -- When the encoding of a USS file is not
                                       able to be converted
        """
        path, dirs, files = next(os.walk(dir_path))
        enc_utils = encode.EncodeUtils()
        for file in files:
            full_file_path = path + "/" + file
            rc = enc_utils.uss_convert_encoding(
                full_file_path, full_file_path, from_code_set, to_code_set
            )
            if not rc:
                raise EncodingConversionError(
                    full_file_path, from_code_set, to_code_set
                )

    def _tag_file_encoding(self, file_path, tag, is_dir=False):
        """Tag the file specified by 'file_path' with the given code set.
        If `file_path` is a directory, all of the files and subdirectories will
        be tagged recursively.

        Arguments:
            file_path {str} -- Absolute file path
            tag {str} -- Specifies which code set to tag the file

        Keyword Arguments:
            is_dir {bool} -- Whether 'file_path' specifies a directory.
                             (Default {False})

        """
        tag_cmd = "chtag -{0}c {1} {2}".format(
            "R" if is_dir else "t", tag, file_path)
        rc, out, err = self.run_command(tag_cmd)
        if rc != 0:
            self.fail_json(
                msg="Unable to tag the file {0} to {1}".format(file_path, tag),
                stdout=out,
                stderr=err,
                rc=rc,
                stdout_lines=out.splitlines(),
                stderr_lines=err.splitlines(),
            )

    def _allocate_ps(self, name, size="5M", alloc_vol=None):
        """Allocate a sequential data set

        Arguments:
            name {str} -- Name of the data set to allocate
            size {str} -- The size to allocate
            alloc_vol {str} -- The volume where the data set should be allocated
        """
        if size is None:
            size = "5M"
        elif size == "":
            size = "5M"

        parms = dict(
            name=name,
            type="SEQ",
            space_primary=size,
            record_format="FB",
            record_length=1028,
            block_size=6144
        )
        if alloc_vol:
            parms['volumes'] = [alloc_vol]

        response = datasets._create(**parms)
        if response.rc != 0:
            self.fail_json(
                msg="Unable to allocate destination data set {0}".format(name),
                rc=response.rc,
                stdout=response.stdout_response,
                stderr=response.stderr_response
            )

    def _merge_hash(self, *args):
        """Combine multiple dictionaries"""
        result = dict()
        for arg in args:
            result.update(arg)
        return result


class USSCopyHandler(CopyHandler):
    def __init__(
        self,
        module,
        dest_exists,
        is_binary=False,
        common_file_args=None,
        backup_name=None,
    ):
        """Utility class to handle copying files or data sets to USS target

        Arguments:
            module {AnsibleModule} -- The AnsibleModule object from currently
                                      running module
            dest_exists {boolean} -- Whether destination already exists

        Keyword Arguments:
            common_file_args {dict} -- mode, group and owner information to be
            applied to destination file.

            is_binary {bool} -- Whether the file to be copied contains binary data
            backup_name {str} -- The USS path or data set name of destination backup
        """
        super().__init__(
            module, dest_exists, is_binary=is_binary, backup_name=backup_name
        )
        self.common_file_args = common_file_args

    def copy_to_uss(
        self,
        src,
        dest,
        conv_path,
        temp_path,
        src_ds_type,
        src_member,
        member_name
    ):
        """Copy a file or data set to a USS location

        Arguments:
            src {str} -- The USS source
            dest {str} -- Destination file or directory on USS
            temp_path {str} -- Path to the location where the control node
                               transferred data to
            conv_path {str} -- Path to the converted source file or directory
            src_ds_type {str} -- Type of source
            src_member {bool} -- Whether src is a data set member
            member_name {str} -- The name of the source data set member

        Returns:
            {str} -- Destination where the file was copied to
        """
        if src_ds_type in MVS_SEQ.union(MVS_PARTITIONED):
            self._mvs_copy_to_uss(
                src, dest, src_ds_type, src_member, member_name=member_name
            )
        else:
            if os.path.isfile(temp_path or conv_path or src):
                dest = self._copy_to_file(src, dest, conv_path, temp_path)
            else:
                dest = self._copy_to_dir(src, dest, conv_path, temp_path)

        if self.common_file_args is not None:
            mode = self.common_file_args.get("mode")
            group = self.common_file_args.get("group")
            owner = self.common_file_args.get("owner")
            if mode is not None:
                self.module.set_mode_if_different(dest, mode, False)
            if group is not None:
                self.module.set_group_if_different(dest, group, False)
            if owner is not None:
                self.module.set_owner_if_different(dest, owner, False)
        return dest

    def _copy_to_file(self, src, dest, conv_path, temp_path):
        """Helper function to copy a USS src to USS dest.

        Arguments:
            src {str} -- USS source file path
            dest {str} -- USS dest file path
            temp_path {str} -- Path to the location where the control node
                               transferred data to
            conv_path {str} -- Path to the converted source file or directory

        Returns:
            {str} -- Destination where the file was copied to
        """
        if os.path.isdir(dest):
            dest = os.path.join(dest, os.path.basename(src)
                                if src else "inline_copy")

        new_src = temp_path or conv_path or src
        try:
            if self.is_binary:
                copy.copy_uss2uss_binary(new_src, dest)
            else:
                shutil.copy(new_src, dest)
        except OSError as err:
            self.fail_json(
                msg="Destination {0} is not writable".format(dest), stderr=str(err)
            )
        except Exception as err:
            self.fail_json(
                msg="Unable to copy file {0} to {1}".format(new_src, dest),
                stderr=str(err),
            )
        return dest

    def _copy_to_dir(self, src_dir, dest_dir, conv_path, temp_path):
        """Helper function to copy a USS directory to another USS directory

        Arguments:
            src_dir {str} -- USS source directory
            dest {str} -- USS dest directory
            temp_path {str} -- Path to the location where the control node
                               transferred data to
            conv_path {str} -- Path to the converted source directory

        Returns:
            {str} -- Destination where the directory was copied to
        """
        new_src_dir = temp_path or conv_path or src_dir
        if os.path.exists(dest_dir):
            try:
                shutil.rmtree(dest_dir)
            except Exception as err:
                self.fail_json(
                    msg="Unable to delete pre-existing directory {0}".format(
                        dest_dir),
                    stdout=str(err),
                )
        try:
            shutil.copytree(new_src_dir, dest_dir)
        except Exception as err:
            self.fail_json(
                msg="Error while copying data to destination directory {0}".format(
                    dest_dir
                ),
                stdout=str(err),
            )
        return dest_dir

    def _mvs_copy_to_uss(
        self,
        src,
        dest,
        src_ds_type,
        src_member,
        member_name=None
    ):
        """Helper function to copy an MVS data set src to USS dest.

        Arguments:
            src {str} -- Name of source data set or data set member
            dest {str} -- USS dest file path
            src_ds_type -- Type of source
            src_member {bool} -- Whether src is a data set member

        Keyword Arguments:
            member_name {str} -- The name of the source data set member
        """
        if os.path.isdir(dest):
            # If source is a data set member, destination file should have
            # the same name as the member.
            dest = "{0}/{1}".format(dest, member_name or src)

            if src_ds_type in MVS_PARTITIONED and not src_member:
                try:
                    os.mkdir(dest)
                except FileExistsError:
                    pass
        try:
            if src_member or src_ds_type in MVS_SEQ:
                response = datasets._copy(src, dest)
                if response.rc != 0:
                    self.fail_json(
                        msg="Error while copying source {0} to {1}".format(
                            src, dest),
                        rc=response.rc,
                        stdout=response.stdout_response,
                        stderr=response.stderr_response
                    )
            else:
                copy.copy_pds2uss(src, dest, is_binary=self.is_binary)
        except Exception as err:
            self.fail_json(msg=str(err))


class PDSECopyHandler(CopyHandler):
    def __init__(
        self,
        module,
        dest_exists,
        is_binary=False,
        backup_name=None
    ):
        """ Utility class to handle copying to partitioned data sets or
        partitioned data set members.

        Arguments:
            module {AnsibleModule} -- The AnsibleModule object from currently
                                      running module
            dest_exists {boolean} -- Whether destination already exists

        Keyword Arguments:
            is_binary {bool} -- Whether the data set to be copied contains
                                binary data
            backup_name {str} -- The USS path or data set name of destination backup
        """
        super().__init__(
            module,
            dest_exists,
            is_binary=is_binary,
            backup_name=backup_name
        )

    def copy_to_pdse(
        self,
        src,
        temp_path,
        conv_path,
        dest,
        src_ds_type,
        alloc_vol=None
    ):
        """Copy source to a PDS/PDSE or PDS/PDSE member.

        Arguments:
            src {str} -- Path to USS file/directory or data set name.
            temp_path {str} -- Path to the location where the control node
                               transferred data to
            conv_path {str} -- Path to the converted source file/directory
            dest {str} -- Name of destination data set
            src_ds_type {str} -- The type of source
            alloc_vol {str} -- The volume where the PDSE should be allocated
        """
        # TODO: review this action which deletes members of a PDS when the PDS is not empty
        #       this seems destructive and unnecessary because a user might be adding members to a pds
        new_src = temp_path or conv_path or src
        if src_ds_type == "USS":
            if self.dest_exists and not data_set.is_empty(dest):
                rc = datasets.delete_members(dest + "(*)")
                if rc != 0:
                    self.fail_json(
                        msg="Unable to delete data set members for data set {0}".format(
                            dest
                        ),
                        rc=rc,
                    )
            if src.endswith("/"):
                new_src = "{0}/{1}".format(
                    temp_path, os.path.basename(os.path.dirname(src))
                )

            path, dirs, files = next(os.walk(new_src))
            for file in files:
                member_name = file[: file.rfind(".")] if "." in file else file
                full_file_path = path + "/" + file
                self.copy_to_member(
                    full_file_path,
                    None,
                    None,
                    "{0}({1})".format(dest, member_name),
                    copy_member=True
                )
        else:
            if is_member_wildcard(src):
                members = []
                data_set_base = data_set.extract_dsname(src)
                try:
                    members = list(map(str.strip, datasets.list_members(src)))
                except AttributeError:
                    self.exit_json(
                        note="The src {0} is likely empty. No data was copied".format(
                            data_set_base
                        )
                    )
                for member in members:
                    self.copy_to_member(
                        "{0}({1})".format(data_set_base, member),
                        None,
                        None,
                        "{0}({1})".format(dest, member),
                    )
            else:
                if self.dest_exists:
                    rc = datasets.delete(dest)
                    if rc != 0:
                        self.fail_json(
                            msg="Error while removing existing destination {0}".format(
                                dest
                            ),
                            rc=rc,
                        )
                    self.allocate_model(dest, new_src, vol=alloc_vol)

                dds = dict(OUTPUT=dest, INPUT=new_src)
                copy_cmd = "   COPY OUTDD=OUTPUT,INDD=INPUT"
                rc, out, err = iebcopy(copy_cmd, dds=dds)
                if rc != 0:
                    self.fail_json(
                        msg="IEBCOPY encountered a problem while copying {0} to {1}".format(
                            new_src, dest
                        ),
                        stdout=out,
                        stderr=err,
                        rc=rc,
                        stdout_lines=out.splitlines(),
                        stderr_lines=err.splitlines(),
                        cmd=copy_cmd,
                    )

    def copy_to_member(
        self,
        src,
        temp_path,
        conv_path,
        dest,
        copy_member=False
    ):
        """Copy source to a PDS/PDSE member. The only valid sources are:
            - USS files
            - Sequential data sets
            - PDS/PDSE members
            - local files

        Arguments:
            src {str} -- Path to USS file or data set name.
            temp_path {str} -- Path to the location where the control node
                               transferred data to
            conv_path {str} -- Path to the converted source file/directory
            dest {str} -- Name of destination data set

        Keyword Arguments:
            copy_member {bool} -- Whether destination specifies a member name.
                                  (default {False})

        Returns:
            {str} -- Destination where the member was copied to
        """
        is_uss_src = temp_path is not None or conv_path is not None or "/" in src
        # if constructing, remove periods from member name
        if src and is_uss_src and not copy_member:
            dest = "{0}({1})".format(
                dest, os.path.basename(src).replace(".", "")[0:8])

        new_src = (temp_path or conv_path or src).replace("$", "\\$")

        dest = dest.replace("$", "\\$").upper()
        opts = dict()

        # added -B to remove the 'truncated' error when copying uss->fb/pdse style stuff
        if self.is_binary:
            opts["options"] = "-B"

        # self.fail_json(
        #             msg="Unable to delete destination data set {0} {1} {2}".format(new_src, dest, opts))

        response = datasets._copy(new_src, dest, None, **opts)
        rc, out, err = response.rc, response.stdout_response, response.stderr_response

        if rc != 0:
            msg = ""
            if is_uss_src:
                msg = "Unable to copy file {0} to data set member {1}".format(
                    src, dest)
            else:
                msg = "Unable to copy data set member {0} to {1}".format(
                    src, dest)

            # *****************************************************************
            # An error occurs while attempting to write a data set member to a
            # PDSE containing program object members, a PDSE cannot contain
            # both program object members and data members. This can be
            # resolved by copying the program object with a "-X" flag.
            # *****************************************************************
            if "FSUM8976" in err and "EDC5091I" in err:
                rc, out, err = self.run_command(
                    "cp -X \"//'{0}'\" \"//'{1}'\"".format(new_src, dest)
                )
                if rc != 0:
                    self.fail_json(msg=msg, rc=rc, stdout=out, stderr=err)
            else:
                self.fail_json(msg=msg, rc=rc, stdout=out, stderr=err)

        return dest.replace("\\", "").upper()

    def create_pdse(
        self,
        src,
        dest_name,
        size,
        src_ds_type,
        remote_src=False,
        src_vol=None,
        alloc_vol=None,
    ):
        """Create a partitioned data set specified by 'dest_name'

        Arguments:
            src {str} -- Name of the source data set
            dest_name {str} -- Name of the data set to be created
            size {int} -- The size, in bytes, of the source file
            dest_ds_type {str} -- Type of the data set to be created
            src_ds_type {str} -- Type of source data set
            alloc_vol {str} -- The volume to allocate the PDSE to

        Keyword Arguments:
            remote_src {bool} -- Whether source is located on remote system.
                                 (Default {False})
            src_vol {str} -- Volume where source data set is stored. (Default {None})
        """
        if remote_src:
            if src_ds_type in MVS_PARTITIONED:
                self.fail_json(
                    msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 333"))
                self.allocate_model(dest_name, src, vol=alloc_vol)

            elif src_ds_type in MVS_SEQ:
                self.fail_json(
                    msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 444"))
                self._allocate_pdse(
                    dest_name, src_vol=src_vol, src=src, alloc_vol=alloc_vol)

            elif os.path.isfile(src):
                self.fail_json(
                    msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 555"))
                size = os.stat(src).st_size
                self._allocate_pdse(dest_name, size=size, alloc_vol=alloc_vol)

            elif os.path.isdir(src):
                self.fail_json(
                    msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 666"))
                path, dirs, files = next(os.walk(src))
                if dirs:
                    self.fail_json(
                        msg="Subdirectory found in source directory {0}".format(
                            src)
                    )
                size = sum(os.stat(path + "/" + f).st_size for f in files)
                self._allocate_pdse(dest_name, size=size, alloc_vol=alloc_vol)
        else:
            #self.fail_json(msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 777"))
            self.fail_json(
                msg="TESTING FAILURE FOR NON-EXISTANT DS TYPE 777 {0} {1} {2}".format(dest_name, size, alloc_vol))
            self._allocate_pdse(dest_name, src=src,
                                size=size, alloc_vol=alloc_vol)

    def _allocate_pdse(
        self,
        ds_name,
        size=None,
        src_vol=None,
        src=None,
        alloc_vol=None
    ):
        """Allocate a partitioned extended data set. If 'size'
        is provided, allocate PDSE using this given size. If size is not
        provided, obtain the 'src' data set size from vtoc and allocate using
        that information.

        Arguments:
            ds_name {str} -- The name of the PDSE to allocate

        Keyword Arguments:
            size {int} -- The size, in bytes, of the allocated PDSE
            src {str} -- The name of the source data set from which to get the size
            src_vol {str} -- Volume of the source data set
            allc_vol {str} -- The volume where PDSE should be allocated
        """
        rc = -1
        recfm = "FB"
        lrecl = 80
        alloc_size = size
        if not alloc_size:
            if src_vol:
                vtoc_info = vtoc.get_data_set_entry(src, src_vol)
                tracks = int(vtoc_info.get("last_block_pointer").get("track"))
                blocks = int(vtoc_info.get("last_block_pointer").get("block"))
                blksize = int(vtoc_info.get("block_size"))
                bytes_per_trk = 56664
                alloc_size = (tracks * bytes_per_trk) + (blocks * blksize)
                recfm = vtoc_info.get("record_format") or recfm
                lrecl = int(vtoc_info.get("record_length")) or lrecl
            else:
                alloc_size = 5242880  # Use the default 5 Megabytes

        alloc_size = "{0}K".format(str(int(math.ceil(alloc_size / 1024))))
        parms = dict(
            name=ds_name,
            type="PDSE",
            space_primary=alloc_size,
            record_format=recfm,
            record_length=lrecl
        )
        if alloc_vol:
            parms['volumes'] = [alloc_vol]

        try:
            rc = data_set.DataSet.create(**parms)
        except Exception as err:
            self.fail_json(msg=repr(err))

        return rc

    # def create_pdse_2(
    #     self,
    #     src,
    #     dest_name,
    #     size,
    #     src_ds_type,
    #     remote_src=False,
    #     src_vol=None,
    #     alloc_vol=None,
    #     space_primary=None,
    #     space_secondary=None,
    #     space_type=None,
    #     record_format=None,
    #     record_length=None,
    #     block_size=None
    # ):
    #     """Create a partitioned data set specified by 'dest_name'

    #     Arguments:
    #         src {str} -- Name of the source data set
    #         dest_name {str} -- Name of the data set to be created
    #         size {int} -- The size, in bytes, of the source file
    #         dest_ds_type {str} -- Type of the data set to be created
    #         src_ds_type {str} -- Type of source data set
    #         alloc_vol {str} -- The volume to allocate the PDSE to

    #     Keyword Arguments:
    #         remote_src {bool} -- Whether source is located on remote system.
    #                              (Default {False})
    #         src_vol {str} -- Volume where source data set is stored. (Default {None})
    #     """
    #     if remote_src:
    #         if src_ds_type in MVS_PARTITIONED:
    #             self.fail_json(
    #                 msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 333"))
    #             self.allocate_model(dest_name, src, vol=alloc_vol)

    #         elif src_ds_type in MVS_SEQ:
    #             self.fail_json(
    #                 msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 444"))
    #             self._allocate_pdse(
    #                 dest_name, src_vol=src_vol, src=src, alloc_vol=alloc_vol)

    #         elif os.path.isfile(src):
    #             # self.fail_json(
    #             #     msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 555"))
    #             size = os.stat(src).st_size
    #             #self._allocate_pdse(dest_name, size=size, alloc_vol=alloc_vol)
    #             self._allocate_pdse_2(dest_name, src=src, size=size, alloc_vol=alloc_vol, space_primary=space_primary,
    #                                   space_secondary=space_secondary,
    #                                   space_type=space_type,
    #                                   record_format=record_format,
    #                                   record_length=record_length,
    #                                   block_size=block_size)

    #         elif os.path.isdir(src):
    #             self.fail_json(
    #                 msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 666"))
    #             path, dirs, files = next(os.walk(src))
    #             if dirs:
    #                 self.fail_json(
    #                     msg="Subdirectory found in source directory {0}".format(
    #                         src)
    #                 )
    #             size = sum(os.stat(path + "/" + f).st_size for f in files)
    #             self._allocate_pdse(dest_name, size=size, alloc_vol=alloc_vol)
    #     else:
    #         #self.fail_json(msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 777"))
    #         # self.fail_json(
    #         #     msg="TESTING FAILURE FOR NON-EXISTANT DS TYPE 777 {0} {1} {2}".format(dest_name, size, alloc_vol))
    #         self._allocate_pdse_2(dest_name, src=src, size=size, alloc_vol=alloc_vol, space_primary=space_primary,
    #                               space_secondary=space_secondary,
    #                               space_type=space_type,
    #                               record_format=record_format,
    #                               record_length=record_length,
    #                               block_size=block_size)

    # def _allocate_pdse_2(
    #     self,
    #     ds_name,
    #     size=None,
    #     src_vol=None,
    #     src=None,
    #     alloc_vol=None,
    #     space_primary=None,
    #     space_secondary=None,
    #     space_type=None,
    #     record_format=None,
    #     record_length=None,
    #     block_size=None
    # ):
    #     """Allocate a partitioned extended data set. If 'size'
    #     is provided, allocate PDSE using this given size. If size is not
    #     provided, obtain the 'src' data set size from vtoc and allocate using
    #     that information.

    #     Arguments:
    #         ds_name {str} -- The name of the PDSE to allocate

    #     Keyword Arguments:
    #         size {int} -- The size, in bytes, of the allocated PDSE
    #         src {str} -- The name of the source data set from which to get the size
    #         src_vol {str} -- Volume of the source data set
    #         allc_vol {str} -- The volume where PDSE should be allocated
    #     """
    #     rc = -1
    #     recfm = "FB"
    #     lrecl = 80
    #     alloc_size = space_primary + space_secondary
    #     if not alloc_size:
    #         if src_vol:
    #             vtoc_info = vtoc.get_data_set_entry(src, src_vol)
    #             tracks = int(vtoc_info.get("last_block_pointer").get("track"))
    #             blocks = int(vtoc_info.get("last_block_pointer").get("block"))
    #             blksize = int(vtoc_info.get("block_size"))
    #             bytes_per_trk = 56664
    #             alloc_size = (tracks * bytes_per_trk) + (blocks * blksize)
    #             recfm = vtoc_info.get("record_format") or recfm
    #             lrecl = int(vtoc_info.get("record_length")) or lrecl
    #         else:
    #             alloc_size = 5242880  # Use the default 5 Megabytes

    #     #alloc_size = "{0}K".format(str(int(math.ceil(alloc_size / 1024))))
    #     # parms = dict(
    #     #     name=ds_name,
    #     #     type="PDSE",
    #     #     space_primary=alloc_size,
    #     #     record_format=recfm,
    #     #     record_length=lrecl
    #     # )
    #     # if alloc_vol:
    #     #     parms['volumes'] = [alloc_vol]

    #     # try:
    #     #     rc = data_set.DataSet.create(**parms)
    #     # except Exception as err:
    #     #     self.fail_json(msg=repr(err))

    #     # return rc

    #     if not self.dest_exists:
    #         parms = dict(
    #             name=ds_name,
    #             type="PDSE",
    #             space_primary=space_primary,
    #             space_secondary=space_secondary,
    #             space_type=space_type,
    #             record_format=record_format,
    #             record_length=record_length,
    #             block_size=block_size
    #         )
    #         if alloc_vol:
    #             parms['volumes'] = [alloc_vol]

    #         try:
    #             data_set.DataSet.create(**parms)
    #         except Exception as err:
    #             self.fail_json(msg=repr(err))

    #     return rc


def create_pdse_2(
    self,
    src,
    dest_name,
    size,
    src_ds_type,
    remote_src=False,
    src_vol=None,
    alloc_vol=None,
    space_primary=None,
    space_secondary=None,
    space_type=None,
    record_format=None,
    record_length=None,
    block_size=None
):
    """Create a partitioned data set specified by 'dest_name'

        Arguments:
            src {str} -- Name of the source data set
            dest_name {str} -- Name of the data set to be created
            size {int} -- The size, in bytes, of the source file
            dest_ds_type {str} -- Type of the data set to be created
            src_ds_type {str} -- Type of source data set
            alloc_vol {str} -- The volume to allocate the PDSE to

        Keyword Arguments:
            remote_src {bool} -- Whether source is located on remote system.
                                 (Default {False})
            src_vol {str} -- Volume where source data set is stored. (Default {None})
    """
    module = AnsibleModuleHelper(argument_spec={})
    if remote_src:
        if src_ds_type in MVS_PARTITIONED:
            module.fail_json(
                    msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 333"))
            allocate_model_data_set(dest_name, src, vol=alloc_vol)

        elif src_ds_type in MVS_SEQ:
            self.fail_json(
                msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 444"))
            self._allocate_pdse(
                dest_name, src_vol=src_vol, src=src, alloc_vol=alloc_vol)

        elif os.path.isfile(src):
            # self.fail_json(
            #     msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 555"))
            size = os.stat(src).st_size
            #self._allocate_pdse(dest_name, size=size, alloc_vol=alloc_vol)
            self._allocate_pdse_2(dest_name, src=src, size=size, alloc_vol=alloc_vol, space_primary=space_primary,
                                    space_secondary=space_secondary,
                                    space_type=space_type,
                                    record_format=record_format,
                                    record_length=record_length,
                                    block_size=block_size)

        elif os.path.isdir(src):
            self.fail_json(
                msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 666"))
            path, dirs, files = next(os.walk(src))
            if dirs:
                self.fail_json(
                    msg="Subdirectory found in source directory {0}".format(
                        src)
                )
            size = sum(os.stat(path + "/" + f).st_size for f in files)
            self._allocate_pdse(dest_name, size=size, alloc_vol=alloc_vol)
        else:
            #self.fail_json(msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 777"))
            # self.fail_json(
            #     msg="TESTING FAILURE FOR NON-EXISTANT DS TYPE 777 {0} {1} {2}".format(dest_name, size, alloc_vol))
            self._allocate_pdse_2(dest_name, src=src, size=size, alloc_vol=alloc_vol, space_primary=space_primary,
                                  space_secondary=space_secondary,
                                  space_type=space_type,
                                  record_format=record_format,
                                  record_length=record_length,
                                  block_size=block_size)


#TODO: Do we really need this when we can use model_ds?
def _allocate_pdse_3(
        self,
        name,
        size=None,
        src=None,
        alloc_vol=None,

    ):
        """Allocate a partitioned extended data set. If 'size'
        is provided, allocate PDSE using this given size. If size is not
        provided, obtain the 'src' data set size from vtoc and allocate using
        that information.

        Arguments:
            name {str} -- The name of the PDSE to allocate

        Keyword Arguments:
            src {str} -- The name of the source data set from which to get the size
            alloc_vol {str} -- The volume where PDSE should be allocated
        """
        # Figure out the data set type and volume
        ds_dataSetUtils = data_set.DataSetUtils(src)

        if ds_dataSetUtils.exists():
            src_vol = ds_dataSetUtils.volume()

        rc = -1
        recfm = "FB"
        lrecl = 80
        if src_vol:
            vtoc_info = vtoc.get_data_set_entry(src, src_vol)
            tracks = int(vtoc_info.get("last_block_pointer").get("track"))
            blocks = int(vtoc_info.get("last_block_pointer").get("block"))
            blksize = int(vtoc_info.get("block_size"))
            bytes_per_trk = 56664
            alloc_size = (tracks * bytes_per_trk) + (blocks * blksize)
            recfm = vtoc_info.get("record_format") or recfm
            lrecl = int(vtoc_info.get("record_length")) or lrecl

        parms = dict(
            name=ds_name,
            type="PDSE",
            space_primary=space_primary,
            space_secondary=space_secondary,
            space_type=space_type,
            record_format=record_format,
            record_length=record_length,
            block_size=block_size
        )

        if alloc_vol:
            parms['volumes'] = [alloc_vol]

        try:
            data_set.DataSet.create(**parms)
        except Exception as err:
            self.fail_json(msg=repr(err))

        # TODO: Correct Return code
        return rc


def _allocate_pdse_2(
        self,
        ds_name,
        size=None,
        src_vol=None,
        src=None,
        alloc_vol=None,
        space_primary=None,
        space_secondary=None,
        space_type=None,
        record_format=None,
        record_length=None,
        block_size=None
    ):
        """Allocate a partitioned extended data set. If 'size'
        is provided, allocate PDSE using this given size. If size is not
        provided, obtain the 'src' data set size from vtoc and allocate using
        that information.

        Arguments:
            ds_name {str} -- The name of the PDSE to allocate

        Keyword Arguments:
            size {int} -- The size, in bytes, of the allocated PDSE
            src {str} -- The name of the source data set from which to get the size
            src_vol {str} -- Volume of the source data set
            allc_vol {str} -- The volume where PDSE should be allocated
        """
        rc = -1
        recfm = "FB"
        lrecl = 80
        alloc_size = space_primary + space_secondary
        if not alloc_size:
            if src_vol:
                vtoc_info = vtoc.get_data_set_entry(src, src_vol)
                tracks = int(vtoc_info.get("last_block_pointer").get("track"))
                blocks = int(vtoc_info.get("last_block_pointer").get("block"))
                blksize = int(vtoc_info.get("block_size"))
                bytes_per_trk = 56664
                alloc_size = (tracks * bytes_per_trk) + (blocks * blksize)
                recfm = vtoc_info.get("record_format") or recfm
                lrecl = int(vtoc_info.get("record_length")) or lrecl
            else:
                alloc_size = 5242880  # Use the default 5 Megabytes

        if not self.dest_exists:
            parms = dict(
                name=ds_name,
                type="PDSE",
                space_primary=space_primary,
                space_secondary=space_secondary,
                space_type=space_type,
                record_format=record_format,
                record_length=record_length,
                block_size=block_size
            )
            if alloc_vol:
                parms['volumes'] = [alloc_vol]

            try:
                data_set.DataSet.create(**parms)
            except Exception as err:
                self.fail_json(msg=repr(err))

        return rc


def allocate_model_data_set(ds_name, model, vol=None):
    """Allocates a data set based on a 'model' data sets allocation paramters.
       Useful when data set needs to be created identical to another. Supported
       model(s) are Physical Sequential (PS) and Partitioned Data Sets PDS. If a
       PDS should have a member PDS(member) set as the `ds_name`, the input will
       be sanitized thus removing the member resulting in only the data set name.

        Arguments:
            ds_name {str} -- The name of the data set to allocate. If the ds_name
            is a partitioned member e.g. hlq.llq.ds(mem), only the data set name
            must be used. See extract_dsname(ds_name) in data_set.py
            model {str} -- The name of the data set whose allocation parameters
            should be used to allocate the new data set 'ds_name'
            vol {str} -- The volume where data set should be allocated

        Returns:
            {int} -- The return code of executing the allocation command
        """

    #TODO: Add a check to ensure ony type PO and PS are provided.
    #TODO: Raise error that incorrect type was passed, only PO/PS are supported
    #TODO: Move this code to shared module utils data set functions
    module = AnsibleModuleHelper(argument_spec={})
    blksize = data_set.DataSetUtils(model).blksize()
    ds_name = data_set.extract_dsname(ds_name)

    alloc_cmd = """  ALLOC DS('{0}') -
    LIKE('{1}') -
    {2}{3}""".format(
          ds_name,
          model,
          "BLKSIZE({0}) ".format(blksize) if blksize else "",
          "VOLUME({0})".format(vol.upper()) if vol else ""
          )

    rc, out, err = ikjeft01(alloc_cmd, authorized=True)
    if rc != 0:
            module.fail_json(
                msg="Unable to allocate destination {0}".format(ds_name),
                stdout=out,
                stderr=err,
                rc=rc,
                stdout_lines=out.splitlines(),
                stderr_lines=err.splitlines(),
                cmd=alloc_cmd,
            )
    return rc


def get_data_set_record_length(name):
    """
    Return the record length of a given data set for PS and PDS/E
    """
    ds_dataSetUtils = data_set.DataSetUtils(name)
    if ds_dataSetUtils.exists():
        ds_vol = ds_dataSetUtils.volume()
        vtoc_info = vtoc.get_data_set_entry(name, ds_vol)
        record_length = int(vtoc_info.get("record_length"))
        return record_length
    return None


def get_file_record_length(name):
    """
    WHAT abuot binary Files, if binary use VB length?
    """


def get_directory_record_length(dir):
    """
    """
    pass


def get_data_set_attributes(name):
    """
    """
    # Figure out the data set type and volume
    ds_dataSetUtils = data_set.DataSetUtils(name)
    if ds_dataSetUtils.exists():
        ds_type = ds_dataSetUtils.ds_type()
        ds_vol = ds_dataSetUtils.volume()

    # Handle the mapping of the real dsorg type to the one used by ZOAU
    if ds_type == "PS":
        ds_type = "SEQ"

    # module = AnsibleModuleHelper(argument_spec={})
    # module.fail_json(msg="Unable to allocate FFFFFFFFFFF {0}".format(ds_dataSetUtils.ds_info))
    # msg": "Unable to allocate FFFFFFFFFFF {'exists': True, 'dsorg': 'PS', 'recfm': 'VB', 'lrecl': 1028, 'blksize': 6144, 'volser': 'DIMATO'}",
            # Possible return values:
            # 'PS'   -- Physical Sequential (SEQ)
            # 'PO'   -- Partitioned (PDS or PDS(E))
            # 'VSAM' -- Virtual Storage Access Method
            # 'DA'   -- Direct Access
            # 'IS'   -- Indexed Sequential
            # 'USS'  -- USS file or directory

    # Figure out the data set attributes
    vtoc_info = vtoc.get_data_set_entry(name, ds_vol)
    if vtoc_info:
        tracks = int(vtoc_info.get("last_block_pointer").get("track"))
        blocks = int(vtoc_info.get("last_block_pointer").get("block"))
        block_size = int(vtoc_info.get("block_size"))
        bytes_per_trk = 56664
        alloc_size = (tracks * bytes_per_trk) + (blocks * block_size)
        record_format = vtoc_info.get("record_format")
        record_length = int(vtoc_info.get("record_length"))
        space_primary = "{0}K".format(str(int(math.ceil(alloc_size / 1024))))
        # TODO: Figure out secondary, default to 3 for now
        space_secondary = "3K"

        if ds_type in MVS_PARTITIONED and data_set.is_empty(name):
            # TODO: Eval why we check is_empty?
            # A general rule is to allow one directory block for every six members in a PDS.
            # TODO: Calculate based on copy member or figure out how to read the director blocks
            #       for now, default to 12.
            directory_blocks = 12
        else:
            # A sequential data set must have a value of 0 in this field
            directory_blocks = 0


    parms = dict(
        name=name,
        type=ds_type,
        space_primary=space_primary,
        space_secondary=space_secondary,
        record_format=record_format,
        record_length=record_length,
        block_size=block_size,
        directory_blocks=directory_blocks
    )

    if ds_vol:
        parms['volumes'] = [ds_vol]

    module = AnsibleModuleHelper(argument_spec={})
    if all(x is None for x in [name, type, space_primary, record_format, record_length, block_size]):
        module.fail_json(msg=repr("Unable to determine data set attributes {0}, {1}, {2}, {3}, {4}, {5}, {6}".format(
            name, type, space_primary, record_format, record_length, block_size, ds_vol)))

    return parms


def get_sequential_data_set_default_attributes(name, size, vol):
    """Provide a sequential data set defaults given size based on bytes
        from a source such as a file.

        TODO: Trying to mock what would be created by USS cp command.
        General Data                           Current Allocation
        Management class . . : **None**        Allocated blocks  . : 8
        Storage class  . . . : **None**        Allocated extents . : 1
            Volume serial . . . : 000000
            Device type . . . . : 3390
        Data class . . . . . : **None**
            Organization  . . . : PS             Current Utilization
            Record format . . . : VB              Used blocks . . . . : 1
            Record length . . . : 1028            Used extents  . . . : 1
            Block size  . . . . : 6144
            1st extent blocks . : 8
            Secondary blocks  . : 24             Dates
            Data set name type  :                 Creation date . . . : 2021/12/04
                                                Referenced date . . : 2021/12/04
                                                Expiration date . . : ***None***
    """
    #2153232
    #2128672
    #2039904 -(36*56,664) https://ibmmainframes.com/references/disk.html
    #space_primary = "{0}K".format(str(int(math.ceil(size / 1024))))
    # Secondary is 10% of primary for defaulted attribute
    #space_secondary = "{0}K".format(str(int(math.ceil((size/1024)*.10))))
    space_primary = int(math.ceil((size/1024)))
    space_primary = space_primary + int(math.ceil(space_primary*.05))
    space_secondary = int(math.ceil(space_primary*.10))
    parms = dict(
        name=name,
        type="SEQ",
        space_primary=space_primary,
        space_secondary=space_secondary,
        record_format="VB",
        record_length=1028,
        block_size=6144,
        space_type="K"
    )

    if vol:
     parms['volumes'] = [vol]

    return parms


def backup_data(ds_name, ds_type, backup_name):
    """Back up the given data set or file to the location specified by 'backup_name'.
    If 'backup_name' is not specified, then calculate a temporary location
    and copy the file or data set there.

    Arguments:
        ds_name {str} -- Name of the file or data set to be backed up
        ds_type {str} -- Type of the file or data set
        backup_name {str} -- Path to USS location or name of data set
        where data will be backed up

    Returns:
        {str} -- The USS path or data set name where data was backed up
    """
    module = AnsibleModuleHelper(argument_spec={})

    try:
        if ds_type == "USS":
            return backup.uss_file_backup(ds_name, backup_name=backup_name)
        return backup.mvs_file_backup(ds_name, backup_name)
    except Exception as err:
        module.fail_json(msg=repr(err))


def is_compatible(src_type, dest_type, copy_member, src_member):
    """Determine whether the src and dest are compatible and src can be
    copied to dest.

    Arguments:
        src_type {str} -- Type of the source (e.g. PDSE, USS)
        dest_type {str} -- Type of destination
        src_member {bool} -- Whether src is a data set member
        copy_member {bool} -- Whether dest is a data set member

    Returns:
        {bool} -- Whether src can be copied to dest
    """
    # ********************************************************************
    # If the destination does not exist, then obviously it will need
    # to be created. As a result, target is compatible.
    # ********************************************************************
    if dest_type is None:
        return True

    # ********************************************************************
    # If source is a sequential data set, then destination must be
    # partitioned data set member, other sequential data sets or USS files.
    # Anything else is incompatible.
    # ********************************************************************
    if src_type in MVS_SEQ:
        return not (
            (dest_type in MVS_PARTITIONED and not copy_member) or dest_type == "VSAM"
        )

    # ********************************************************************
    # If source is a partitioned data set, then we need to determine
    # target compatibility for two different scenarios:
    #   - If the source is a data set member
    #   - If the source is an entire data set
    #
    # In the first case, the possible targets are: USS files, PDS/PDSE
    # members and sequential data sets. Anything else is incompatible.
    #
    # In the second case, the possible targets are USS directories and
    # other PDS/PDSE. Anything else is incompatible.
    # ********************************************************************
    elif src_type in MVS_PARTITIONED:
        if dest_type == "VSAM":
            return False
        if not src_member:
            return not (copy_member or dest_type in MVS_SEQ)
        return True

    elif src_type == "USS":
        return dest_type != "VSAM"

    else:
        return dest_type == "VSAM"


def get_file_checksum(src):
    """Calculate SHA256 hash for a given file

    Arguments:
        src {str} -- The absolute path of the file

    Returns:
        str -- The SHA256 hash of the contents of input file
    """
    b_src = to_bytes(src)
    if not os.path.exists(b_src) or os.path.isdir(b_src):
        return None
    blksize = 64 * 1024
    hash_digest = sha256()
    try:
        with open(to_bytes(src, errors="surrogate_or_strict"), "rb") as infile:
            block = infile.read(blksize)
            while block:
                hash_digest.update(block)
                block = infile.read(blksize)
    except Exception:
        raise
    return hash_digest.hexdigest()


def cleanup(src_list):
    """Remove all files or directories listed in src_list. Also perform
    additional cleanup of the /tmp directory.

    Arguments:
        src_list {list} -- A list of file paths
    """
    module = AnsibleModuleHelper(argument_spec={})
    tmp_prefix = tempfile.gettempprefix()
    tmp_dir = os.path.realpath("/" + tmp_prefix)
    dir_list = glob.glob(tmp_dir + "/ansible-zos-copy-payload*")
    conv_list = glob.glob(tmp_dir + "/converted*")
    tmp_list = glob.glob(tmp_dir + "/{0}*".format(tmp_prefix))
    tmp_ds = glob.glob(tmp_dir + "/*.*.*.*")

    for file in (dir_list + conv_list + tmp_list + tmp_ds + src_list):
        try:
            if file and os.path.exists(file):
                if os.path.isfile(file):
                    os.remove(file)
                else:
                    shutil.rmtree(file)
        except OSError as err:
            err = str(err)
            if "Permission denied" not in err:
                module.fail_json(
                    msg="Error during clean up of file {0}".format(file), stderr=err
                )


def is_member_wildcard(src):
    """Determine whether src specifies a data set member wildcard in the
    form 'SOME.DATA.SET(*)' or 'SOME.DATA.SET(ABC*)'

    Arguments:
        src {str} -- The data set name

    Returns:
        re.Match -- If the data set specifies a member wildcard
        None -- If the data set does not specify a member wildcard
    """
    return fullmatch(
        r"^(?:(?:[A-Z$#@]{1}[A-Z0-9$#@-]{0,7})(?:[.]{1})){1,21}[A-Z$#@]{1}[A-Z0-9$#@-]{0,7}\([A-Z$#@\*]{1}[A-Z0-9$#@\*]{0,7}\)$",
        src,
        IGNORECASE,
    )


def is_destination_dataset_user_provided(
    type,
    space_primary,
    space_secondary,
    space_type,
    record_format,
    record_length,
    block_size
    # volume
):
    """Helper method to return if the user has provided the destination data set args

    Args:
        type (str, required): The type of dataset.
        space_primary (int, required): The amount of primary space to allocate for the dataset.
        space_secondary (int, optional):  The amount of secondary space to allocate for the dataset.
        space_type (str, required): The unit of measurement to use when defining primary and secondary space.
        record_format (str, optional): The record format to use for the dataset.
        record_length (int, optional) The length, in bytes, of each record in the data set.
        block_size (int, optional): The block size to use for the data set.
        volume (str, optional): A volume serial.
        """
    if type and space_primary and space_type:
        return True
    else:
        module = AnsibleModuleHelper(argument_spec={})
        # module.fail_json(msg="Error during clean up of file {0}".format(space_primary), stderr=destination_dataset)
        module.fail_json(msg="Error during clean up of file {0}, {1}, {2}, {3}, {4}, {5}, {6}".format(
            type, space_primary, space_secondary, space_type, record_format, record_length, block_size))
    return False


def run_module(module, arg_def):
    # ********************************************************************
    # Verify the validity of module args. BetterArgParser raises ValueError
    # when a parameter fails its validation check
    # ********************************************************************
    try:
        parser = better_arg_parser.BetterArgParser(arg_def)
        parser.parse_args(module.params)
    except ValueError as err:
        # Bypass BetterArgParser when src is of the form 'SOME.DATA.SET(*)'
        if not is_member_wildcard(module.params["src"]):
            module.fail_json(
                msg="Parameter verification failed", stderr=str(err))
    # ********************************************************************
    # Initialize module variables
    # ********************************************************************
    src = module.params.get('src')
    dest = module.params.get('dest')
    remote_src = module.params.get('remote_src')
    is_binary = module.params.get('is_binary')
    backup = module.params.get('backup')
    backup_name = module.params.get('backup_name')
    validate = module.params.get('validate')
    mode = module.params.get('mode')
    group = module.params.get('group')
    owner = module.params.get('owner')
    encoding = module.params.get('encoding')
    volume = module.params.get('volume')
    is_uss = module.params.get('is_uss')
    is_pds = module.params.get('is_pds')
    is_mvs_dest = module.params.get('is_mvs_dest')
    temp_path = module.params.get('temp_path')
    alloc_size = module.params.get('size')
    src_member = module.params.get('src_member')
    copy_member = module.params.get('copy_member')
    force = module.params.get('force')

    destination_dataset = module.params.get('destination_dataset')
    if destination_dataset:
        type = destination_dataset.get("type")
        space_primary = destination_dataset.get("space_primary")
        space_secondary = destination_dataset.get("space_secondary")
        space_type = destination_dataset.get("space_type")
        record_format = destination_dataset.get("record_format")
        record_length = destination_dataset.get("record_length")
        block_size = destination_dataset.get("block_size")

        if volume:
            (module.params.get('destination_dataset'))["volumes"] = [volume]

    # ********************************************************************
    # When copying to and from a data set member, 'dest' or 'src' will be
    # in the form DATA.SET.NAME(MEMBER). When this is the case, extract the
    # actual name of the data set.
    # ********************************************************************
    dest_name = data_set.extract_dsname(dest)
    dest_member = data_set.extract_member_name(dest) if copy_member else None
    src_name = data_set.extract_dsname(src) if src else None
    member_name = data_set.extract_member_name(src) if src_member else None

    conv_path = None
    src_ds_vol = src_ds_type = dest_ds_type = dest_exists = None
    res_args = dict()

    # ********************************************************************
    # 1. When the source is a USS file or directory , verify that the file
    #    or directory exists and has proper read permissions.
    # 2. Capture the file or data sets mode bits when mode param is set
    #    to 'preserve'
    # ********************************************************************
    if remote_src and "/" in src:
        src = os.path.realpath(src)
        if not os.path.exists(src):
            module.fail_json(msg="Source {0} does not exist".format(src))
        if not os.access(src, os.R_OK):
            module.fail_json(msg="Source {0} is not readable".format(src))
        if mode == "preserve":
            mode = "0{0:o}".format(stat.S_IMODE(os.stat(src).st_mode))

    # ********************************************************************
    # 1. Use DataSetUtils to determine the src and dest data set type.
    # 2. For source data sets, find its volume, which will be used later.
    # ********************************************************************
    try:
        if is_uss:
            dest_ds_type = "USS"
            dest_exists = os.path.exists(dest)
        else:
            dest_du = data_set.DataSetUtils(dest_name)
            dest_exists = dest_du.exists()
            if copy_member:
                dest_exists = dest_exists and dest_du.member_exists(
                    dest_member)
            dest_ds_type = dest_du.ds_type()

            # destination_dataset.type overrides `dest_ds_type` given precedence rules
            if destination_dataset and type is not None:
                dest_ds_type = type

        # If temp_path; the plugin has copied a file from controller to USS for localized operation
        if temp_path or "/" in src:
            src_ds_type = "USS"
        else:
            src_du = data_set.DataSetUtils(src_name)
            if src_du.exists():
                if src_member and not src_du.member_exists(member_name):
                    raise NonExistentSourceError(src)
                src_ds_type = src_du.ds_type()
                src_ds_vol = src_du.volume()
            else:
                raise NonExistentSourceError(src)

    except Exception as err:
        module.fail_json(msg=str(err))

    # ********************************************************************
    # Some src and dest combinations are incompatible. For example, it is
    # not possible to copy a PDS member to a VSAM data set or a USS file
    # to a PDS. Perform these sanity checks.
    # Note: dest_ds_type can also be passed from destination_dataset.type
    # ********************************************************************
    if not is_compatible(src_ds_type, dest_ds_type, copy_member, src_member):
        module.fail_json(
            msg="Incompatible target type '{0}' for source '{1}'".format(
                dest_ds_type, src_ds_type
            )
        )

    # ********************************************************************
    # Backup should only be performed if dest is an existing file or
    # data set. Otherwise ignored.
    #
    # If destination exists and the 'force' parameter is set to false,
    # the module exits with a note to the user.
    # ********************************************************************
    if dest_exists:
        if backup or backup_name:
            if dest_ds_type in MVS_PARTITIONED and data_set.is_empty(dest_name):
                # The partitioned data set is empty
                res_args["note"] = "Destination is empty, backup request ignored"
            else:
                backup_name = backup_data(dest, dest_ds_type, backup_name)
    # ********************************************************************
    # If destination does not exist, it must be created. To determine
    # what type of data set destination must be, a couple of simple checks
    # can be done. For example:
    # 1. Destination must be a PDS/PDSE if:
    #   - The source is a local directory
    #   - The source is a USS directory
    #   - The source is a PDS/PDSE
    #   - The destination is in the form DATA.SET.NAME(MEMBER)
    #
    # USS files and sequential data sets are not required to be explicitly
    # created; they are automatically created by the Python/ZOAU API.
    #
    # is_pds is determined in the plugin when src is a directory and destination
    # is data set (is_src_dir and is_mvs_dest)
    # ********************************************************************
    else:
        if not dest_ds_type:
            if (
                is_pds
                or copy_member
                or (src_ds_type in MVS_PARTITIONED and (not src_member) and is_mvs_dest)
                or (src and os.path.isdir(src) and is_mvs_dest)
            ):
                dest_ds_type = "PDSE"
            elif src_ds_type == "VSAM":
                dest_ds_type = "VSAM"
            elif not is_uss:
                dest_ds_type = "SEQ"

        res_args["changed"] = True

    # ********************************************************************
    # Data set `dest` entries will be created under these precedent rules:
    #
    # - IF `dest` is a data set (data set name):
    #   1) If `destination_dataset` values, values will be used to create/replace `dest`
    #   2) If `force` = `true` and `dest` is NOT a (PDS(member)/PDSE(Member), VSAM, USS target) and `src` is NOT VSAM:
    #      2.0) If `dest` data set exists, is empty, `dest` data set will be used
    #      2.1) If `src` data set exists, `src` attributes will be used to create/replace `dest`
    #      2.2) If `src` is a file and `dest` data set exists, `dest` attributes will be used to create data set
    #      2.3) if `src` is a file and `dest` data set does NOT exist, default values will be provided (that may not be suitably enough)
    #   3) If `force` = `true` and `dest` IS a PDS/  Member and NOT (VSAM, USS target) and `src` is NOT VSAM and `src` is a Partitioned Data Set(member)/PDS(member):
    #      3.0) If `dest` data set exists, `dest` is Physical Sequencial (PS), is empty, `dest` data set will be used for the Member (space is not validated, future? )
    #      3.1) If `dest` data set exists, `dest` is Physical Sequencial (PS), is NOT empty, `dest` attributes will be used to create/replace `dest`
    #      3.2) If `dest` data set does NOT exist, `dest` data set name is NOT a PDS(member), `dest` is not a USS target, a default Physical Sequencial (PS) will be created
    #      3.3) If `dest` data set does NOT exist, `dest` data set name IS a PDS(member), `src` is existing PDS(member), `src` attributes will be used to create `dest`
    #      3.4) If `dest` data set does NOT exist, `dest` data set name IS a PDS(member), `src` is NOT existing PDS(member) AKA... FILE, a defaulted Partitioned Data Set(member) will be created
    # - When `force` = `false`:
    #   1) If `dest` exists, program will terminate with message else logic when `force` = `true` is honored
    # Notes: if dest is a member, and dest does not exist,
    # ********************************************************************

    # Creating the destination when is dataset that is not a VSAM.
    if not is_uss and src_ds_type != "VSAM" and (not dest_exists or force):
        # For now, treating SEQ and PDS datasets completely differently.
        # Replacing an existing dataset only when it's not empty. We don't know whether that
        # empty dataset was created for the user by an admin/operator, and they don't have permissions
        # to create new datasets.
        if not dest_member and not (dest_exists and data_set.is_empty(dest_name)):
            dest_params = dict()

            if destination_dataset: # (1) THIS IS EXERCISED for DS new, DS to DS
                if not dest_name:
                    module.fail_json(
                        msg="No qualifying destination data set name provided, correct the value for option 'dest'."
                    )

                dest_params = module.params.get("destination_dataset")
                dest_params["name"] = dest_name
            elif src_ds_type == "USS" and dest_exists: #(2.2) X  -here binary won't matter we rely on what the user gave us
                dest_params = get_data_set_attributes(dest_name)
            elif src_ds_type == "USS" and not dest_exists: #(2.3) X - need to account for binary and also for recl
                new_src = temp_path or src # Taking the temp file when a local file was copied with sftp.
                src_name_file_size_bytes = os.stat(new_src).st_size
                dest_params = get_sequential_data_set_default_attributes(
                    name=dest_name, size=src_name_file_size_bytes, vol=volume)

            if data_set.DataSet.data_set_exists(src_name):  #(2.1) X
                allocate_model_data_set(ds_name=dest_name, model=src_name, vol=volume)
            else:
                data_set.DataSet.ensure_present(replace=force, **dest_params)
        # TODO: see if I can refactor the PDS creation into the above block and then follow up with member creation.
        # elif dest_member and (src_ds_type != "VSAM" or dest_ds_type != "VSAM") and (dest_ds_type != "USS") and force:
        #     if data_set.DataSet.data_set_exists(dest_name, volume) and (dest_ds_type == "PS") and data_set.is_empty(dest_name): #3.0
        #         pass
        #     elif data_set.DataSet.data_set_exists(dest_name, volume) and (dest_ds_type == "PS") and (not data_set.is_empty(dest_name)): # 3.1
        #         dest_parms = get_data_set_attributes(dest_name)
        #         data_set.DataSet.ensure_present(replace=force, **dest_parms)
        #     elif (not data_set.DataSet.data_set_exists(dest_name) and (not data_set.DataSet.is_member(dest_name) and (dest_ds_type != "USS"))):  #3.2

        #         if data_set.DataSet.is_member(src):  # we don't know if the dest is a pds/pdse based on the
        #             src_name_cat = subprocess.run(["cat","//'" + src_name + "'"],check=True, capture_output=True, shell=False)
        #             src_name_wc = subprocess.run(['wc','-c'],input=src_name_cat.stdout, capture_output=True)
        #             src_name_bytes=src_name_wc.stdout.strip()
        #             src_name_bytes_str=src_name_bytes.decode('utf-8')
        #             dest_name_size_bytes=int(src_name_bytes_str)

        #             dest_parms = get_sequential_data_set_default_attributes(
        #                 name=dest_name, size=dest_name_size_bytes, vol=volume)
        #             data_set.DataSet.ensure_present(replace=force, **dest_parms)


        #         elif is_member_wildcard(src): #Src has many members create a copy of it as the dest
        #             allocate_model_data_set(dest_name, src_name, vol=alloc_vol)
        #         elif os.path.isfile(src):
        #             size = os.stat(src).st_size
        #             #self._allocate_pdse(dest_name, size=size, alloc_vol=alloc_vol)
        #             _allocate_pdse_2(dest_name, src=src, size=size, alloc_vol=alloc_vol, space_primary=space_primary,
        #                                   space_secondary=space_secondary,
        #                                   space_type=space_type,
        #                                   record_format=record_format,
        #                                   record_length=record_length,
        #                                   block_size=block_size)

        #             #TODO: What about using [awk '{ if ( length > L ) { L=length} }END{ print L}' ./*] to find max line length for recl
        #             # awk '{ if ( length > L ) { L=length} }END{ print L}' "//'IMSTESTL.DIMATO.JCL(RACFCMD)'"
        #             # 80
        #         elif os.path.isdir(src):
        #             path, dirs, files = next(os.walk(src))
        #             if dirs:
        #                 module.fail_json(
        #                     msg="Subdirectory found in source directory {0}".format(
        #                         src)
        #                 )
        #             size = sum(os.stat(path + "/" + f).st_size for f in files)
        #             _allocate_pdse(dest_name, size=size, alloc_vol=alloc_vol)
        #     elif not data_set.DataSet.data_set_exists(dest_name) and data_set.DataSet.is_member(dest_name) and (src_ds_type == "PO"): #3.3
        #         allocate_model_data_set(dest_name, src_name, vol=alloc_vol)
        #         #TODO: Do we allocate the members?
        #         #      What if they used a wild card PDS(*) do we let the handler create the members?
        #     elif not data_set.DataSet.data_set_exists(dest_name) and data_set.DataSet.is_member(dest_name) and (not data_set.DataSet.data_set_exists(src_name)): #3.4
        #         # TODO: this is the case src is a file or a directory what do we do here? Create a PDSE based on size src and then create all the members or pass completely
        #         ## data_set.DataSet.ensure_member_present(member_name ,replace=force)
        #         pass
        #     elif os.path.isfile(src):
        #         pass
        #         ## same as 3.4

        #         # //If src is a directory, destination must be a partitioned data set or a USS directory.
        #         # //Wildcards can be used to copy multiple PDS/PDSE members to another PDS/PDSE.
        else:
            module.fail_json(
                msg="Failed precedence rules, something slipped through the crack"
            )

    # ********************************************************************
    # Encoding conversion is only valid if the source is a local file,
    # local directory or a USS file/directory.
    # ********************************************************************
    copy_handler = CopyHandler(
        module,
        dest_exists,
        is_binary=is_binary,
        backup_name=backup_name
    )
    if encoding:
        if remote_src and src_ds_type != "USS":
            copy_handler.fail_json(
                msg="Encoding conversion is only valid for USS source"
            )
        # 'conv_path' points to the converted src file or directory
        if is_mvs_dest:
            encoding["to"] = encode.Defaults.DEFAULT_EBCDIC_MVS_CHARSET

        conv_path = copy_handler.convert_encoding(src, temp_path, encoding)

    # ------------------------------- o -----------------------------------
    # Copy to USS file or directory
    # ---------------------------------------------------------------------
    if is_uss:
        if dest_exists and not os.access(dest, os.W_OK):
            copy_handler.fail_json(
                msg="Destination {0} is not writable".format(dest))

        uss_copy_handler = USSCopyHandler(
            module,
            dest_exists,
            is_binary=is_binary,
            common_file_args=dict(mode=mode, group=group, owner=owner),
            backup_name=backup_name,
        )
        dest = uss_copy_handler.copy_to_uss(
            src,
            dest,
            conv_path,
            temp_path,
            src_ds_type,
            src_member,
            member_name
        )
        res_args['size'] = os.stat(dest).st_size
        remote_checksum = dest_checksum = None
        if validate:
            try:
                remote_checksum = get_file_checksum(temp_path or src)
                dest_checksum = get_file_checksum(dest)
            except Exception as err:
                copy_handler.fail_json(
                    msg="Unable to calculate checksum", stderr=str(err)
                )
            res_args["checksum"] = remote_checksum
            res_args["changed"] = (
                res_args.get("changed") or remote_checksum != dest_checksum
            )
        else:
            try:
                remote_checksum = get_file_checksum(temp_path or src)
                dest_checksum = get_file_checksum(dest)
            except Exception as err:
                pass
            res_args["changed"] = (
                res_args.get("changed") or remote_checksum != dest_checksum
            )

    # ------------------------------- o -----------------------------------
    # Copy to sequential data set (PS / SEQ)
    # ---------------------------------------------------------------------
    elif dest_ds_type in MVS_SEQ:
        copy_handler.copy_to_seq(
            src,
            temp_path,
            conv_path,
            dest,
            src_ds_type,
        )
        dest = dest.upper()

    # ---------------------------------------------------------------------
    # Copy to PDS/PDSE
    # ---------------------------------------------------------------------
    elif dest_ds_type in MVS_PARTITIONED:
        if not remote_src and not copy_member and os.path.isdir(temp_path):
            temp_path = os.path.join(temp_path, os.path.basename(src))

        pdse_copy_handler = PDSECopyHandler(
            module, dest_exists, is_binary=is_binary, backup_name=backup_name
        )

        if copy_member or os.path.isfile(temp_path or src) or src_member:
            #module.fail_json(msg=repr("TESTING FAILURE FOR NON-EXISTANT DS TYPE 2222"))
            dest = pdse_copy_handler.copy_to_member(
                src,
                temp_path,
                conv_path,
                dest,
                copy_member=copy_member
            )
        else:
            pdse_copy_handler.copy_to_pdse(
                src, temp_path, conv_path, dest, src_ds_type, alloc_vol=volume
            )
        dest = dest.upper()

    # ------------------------------- o -----------------------------------
    # Copy to VSAM data set
    # ---------------------------------------------------------------------
    else:
        copy_handler.copy_to_vsam(src, dest, alloc_vol=volume)

    res_args.update(
        dict(
            src=src,
            dest=dest,
            ds_type=dest_ds_type,
            dest_exists=dest_exists,
            backup_name=backup_name,
        )
    )

    return res_args, temp_path, conv_path


def main():
    module = AnsibleModule(
        argument_spec=dict(
            src=dict(type='path'),
            dest=dict(required=True, type='str'),
            is_binary=dict(type='bool', default=False),
            encoding=dict(type='dict'),
            content=dict(type='str', no_log=True),
            backup=dict(type='bool', default=False),
            backup_name=dict(type='str'),
            local_follow=dict(type='bool', default=True),
            remote_src=dict(type='bool', default=False),
            sftp_port=dict(type='int', required=False),
            ignore_sftp_stderr=dict(type='bool', default=False),
            validate=dict(type='bool', default=False),
            volume=dict(type='str', required=False),
            destination_dataset=dict(
                type='dict',
                required=False,
                options=dict(
                    type=dict(
                        arg_type='str',
                        choices=['BASIC', 'KSDS', 'ESDS', 'RRDS',
                                 'LDS', 'SEQ', 'PDS', 'PDSE', 'MEMBER'],
                        required=False,
                    ),
                    space_primary=dict(
                        arg_type='int', required=False),
                    space_secondary=dict(
                        arg_type='int', required=False),
                    space_type=dict(
                        arg_type='str',
                        choices=['K', 'M', 'G', 'CYL', 'TRK'],
                        required=False,
                    ),
                    record_format=dict(
                        arg_type='str',
                        choices=["FB", "VB", "FBA", "VBA", "U"],
                        required=False
                    ),
                    record_length=dict(type='int', required=False),
                    block_size=dict(type='int', required=False),
                )
            ),
            is_uss=dict(type='bool'),
            is_pds=dict(type='bool'),
            is_mvs_dest=dict(type='bool'),
            size=dict(type='int'),
            temp_path=dict(type='str'),
            copy_member=dict(type='bool'),
            src_member=dict(type='bool'),
            local_charset=dict(type='str'),
            force=dict(type='bool', default=False)
        ),
        add_file_common_args=True,
    )
    if module.params.get('sftp_port'):
        module.deprecate(
            msg='Support for configuring sftp_port has been deprecated.'
            'Configuring the SFTP port is now managed through Ansible connection plugins option \'ansible_port\'',
            date='2021-08-01', collection_name='ibm.ibm_zos_core')
        # Date and collection are supported in Ansbile 2.9.10 or later

    arg_def = dict(
        src=dict(arg_type='data_set_or_path', required=False),
        dest=dict(arg_type='data_set_or_path', required=True),
        is_binary=dict(arg_type='bool', required=False, default=False),
        content=dict(arg_type='str', required=False),
        backup=dict(arg_type='bool', default=False, required=False),
        backup_name=dict(arg_type='data_set_or_path', required=False),
        local_follow=dict(arg_type='bool', default=True, required=False),
        remote_src=dict(arg_type='bool', default=False, required=False),
        checksum=dict(arg_type='str', required=False),
        validate=dict(arg_type='bool', required=False),
        sftp_port=dict(arg_type='int', required=False),
        volume=dict(arg_type='str', required=False),

        destination_dataset=dict(
            arg_type='dict',
            required=False,
            options=dict(
                type=dict(arg_type='str', required=False),
                space_primary=dict(arg_type='int', required=False),
                space_secondary=dict(
                    arg_type='int', required=False),
                space_type=dict(arg_type='str', required=False),
                record_format=dict(
                    arg_type='str', required=False),
                block_size=dict(arg_type='int', required=False),
            )
        ),
    )

    # Get the users spec early before any defaults are applied (if even defaults will continue) but more so
    # we want to know if a user has provided us `destination_dataset` values so order of precedence can be honored
    if module.params.get('destination_dataset'):
        # Temp work around
        is_destination_dataset_provided = is_destination_dataset_user_provided(
            **module.params.get('destination_dataset'))

    if (
        not module.params.get("encoding")
        and not module.params.get("remote_src")
        and not module.params.get("is_binary")
    ):
        module.params["encoding"] = {
            "from": module.params.get("local_charset"),
            "to": encode.Defaults.get_default_system_charset(),
        }

    if module.params.get("encoding"):
        module.params.update(
            dict(
                from_encoding=module.params.get("encoding").get("from"),
                to_encoding=module.params.get("encoding").get("to"),
            )
        )
        arg_def.update(
            dict(
                from_encoding=dict(arg_type="encoding"),
                to_encoding=dict(arg_type="encoding"),
            )
        )

    res_args = temp_path = conv_path = None
    try:
        res_args, temp_path, conv_path = run_module(module, arg_def)
        module.exit_json(**res_args)
    finally:
        cleanup([temp_path, conv_path])


class EncodingConversionError(Exception):
    def __init__(self, src, f_code, t_code):
        self.msg = "Unable to convert encoding for {0} from {1} to {2}".format(
            src, f_code, t_code
        )
        super().__init__(self.msg)


class NonExistentSourceError(Exception):
    def __init__(self, src):
        self.msg = "Source {0} does not exist".format(src)
        super().__init__(self.msg)


if __name__ == "__main__":
    main()
